"""
Tokenè®¡æ•°æ¨¡å—
æ”¯æŒå¤šç§æ¨¡å‹çš„ç²¾ç¡®tokenè®¡æ•°ï¼Œä½¿ç”¨å®˜æ–¹åˆ†è¯å™¨
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
import json
import os

logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡å­˜å‚¨tokenizerå®ä¾‹
_tiktoken_cache = {}
_anthropic_client = None
_gemini_model = None  # Geminiæ¨¡å‹å®ä¾‹ï¼ˆç”¨äºtokenè®¡æ•°ï¼‰
_gemma_tokenizer = None  # Gemma tokenizerï¼ˆç”¨äºGemini tokenè®¡æ•°ï¼‰
_deepseek_tokenizer = None  # DeepSeek tokenizerï¼ˆç”¨äºDeepSeek tokenè®¡æ•°ï¼‰

# é»˜è®¤tokenizeré…ç½®ï¼ˆå¦‚æœconfig.jsoncä¸­æ²¡æœ‰é…ç½®ï¼‰
DEFAULT_TOKENIZER_CONFIG = {
    "claude": "anthropic",
    "claude-3": "anthropic",
    "claude-3-opus": "anthropic",
    "claude-3-sonnet": "anthropic",
    "claude-3-haiku": "anthropic",
    "claude-3.5-sonnet": "anthropic",
    "gemini": "google",
    "gemini-pro": "google",
    "gemini-ultra": "google",
    "gemini-1.5": "google",
    "gemini-2": "google",
    "gpt-4": "tiktoken",
    "gpt-3.5": "tiktoken",
    "gpt-4-turbo": "tiktoken",
    "chatgpt": "tiktoken",
    "deepseek": "deepseek",
    "deepseek-chat": "deepseek",
    "deepseek-coder": "deepseek",
    "deepseek-v3": "deepseek"
}

# ç¼“å­˜çš„tokenizeré…ç½®
_tokenizer_config = None

# æ¨¡å‹tokenå€æ•°æ ¡å‡†ç³»æ•°ï¼ˆç›¸å¯¹äºGPT-4çš„cl100k_baseï¼‰
# åŸºå‡†ï¼šGPT-4 = 1.0
MODEL_TOKEN_MULTIPLIERS = {
    # Claudeç³»åˆ—ï¼šçº¦ä¸ºGPT-4çš„1.0å€ï¼ˆä½¿ç”¨ç›¸åŒçš„cl100k_baseä½œä¸ºåŸºå‡†ï¼‰
    'claude': 1.0,
    'claude-3': 1.0,
    'claude-3-opus': 1.0,
    'claude-3-sonnet': 1.0,
    'claude-3-haiku': 1.0,
    'claude-3.5-sonnet': 1.0,
    
    # Geminiç³»åˆ—ï¼šçº¦ä¸ºClaudeçš„0.625å€ï¼ˆå³Claudeæ˜¯Geminiçš„1.6å€ï¼‰
    # 0.625 = 1 / 1.6
    'gemini': 0.625,
    'gemini-pro': 0.625,
    'gemini-ultra': 0.625,
    'gemini-1.5': 0.625,
    'gemini-2': 0.625,
    
    # GPTç³»åˆ—ï¼šåŸºå‡†å€¼
    'gpt-4': 1.0,
    'gpt-3.5': 1.0,
    'gpt-4-turbo': 1.0,
    'chatgpt': 1.0,
}

def get_model_multiplier(model_name: str) -> float:
    """
    è·å–æ¨¡å‹çš„tokenå€æ•°æ ¡å‡†ç³»æ•°
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        æ ¡å‡†ç³»æ•°ï¼ˆé»˜è®¤1.0ï¼‰
    """
    if not model_name:
        return 1.0
    
    model_lower = model_name.lower()
    
    # ç²¾ç¡®åŒ¹é…
    if model_lower in MODEL_TOKEN_MULTIPLIERS:
        return MODEL_TOKEN_MULTIPLIERS[model_lower]
    
    # æ¨¡ç³ŠåŒ¹é…
    for key, multiplier in MODEL_TOKEN_MULTIPLIERS.items():
        if key in model_lower:
            return multiplier
    
    # é»˜è®¤è¿”å›1.0
    return 1.0

def load_tokenizer_config() -> Dict[str, str]:
    """
    ä»config.jsoncåŠ è½½tokenizeré…ç½®
    
    Returns:
        tokenizeré…ç½®å­—å…¸
    """
    global _tokenizer_config
    
    if _tokenizer_config is not None:
        return _tokenizer_config
    
    try:
        # å°è¯•ä»å·²åŠ è½½çš„CONFIGä¸­è·å–ï¼ˆé¿å…é‡å¤è§£æJSONCï¼‰
        from core.config_loader import CONFIG
        if CONFIG and 'tokenizer_config' in CONFIG:
            _tokenizer_config = CONFIG['tokenizer_config']
            logger.info(f"[TOKEN_COUNTER] å·²ä»CONFIGåŠ è½½tokenizeré…ç½®ï¼Œå…±{len(_tokenizer_config)}ä¸ªæ¨¡å‹æ˜ å°„")
            return _tokenizer_config
    except Exception as e:
        logger.debug(f"[TOKEN_COUNTER] ä»CONFIGåŠ è½½å¤±è´¥: {e}")
    
    try:
        # å›é€€ï¼šä½¿ç”¨config_loaderçš„_parse_jsoncæ¥æ­£ç¡®è§£æJSONC
        from core.config_loader import _parse_jsonc
        config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.jsonc')
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                content = f.read()
                config = _parse_jsonc(content)
                _tokenizer_config = config.get('tokenizer_config', DEFAULT_TOKENIZER_CONFIG)
                logger.info(f"[TOKEN_COUNTER] å·²åŠ è½½tokenizeré…ç½®ï¼Œå…±{len(_tokenizer_config)}ä¸ªæ¨¡å‹æ˜ å°„")
                return _tokenizer_config
    except Exception as e:
        logger.warning(f"[TOKEN_COUNTER] åŠ è½½tokenizeré…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    
    _tokenizer_config = DEFAULT_TOKENIZER_CONFIG
    return _tokenizer_config

def get_deepseek_tokenizer():
    """
    è·å–DeepSeek tokenizerå®ä¾‹
    ä¼˜å…ˆä»æœ¬åœ°deepseek_v3_tokenizerç›®å½•åŠ è½½
    
    Returns:
        DeepSeek tokenizerå®ä¾‹æˆ–None
    """
    global _deepseek_tokenizer
    
    if _deepseek_tokenizer is not None:
        return _deepseek_tokenizer
    
    try:
        from transformers import AutoTokenizer
        import warnings
        
        # å¿½ç•¥PyTorch/TensorFlowæœªå®‰è£…çš„è­¦å‘Š
        warnings.filterwarnings('ignore', message='.*PyTorch.*')
        warnings.filterwarnings('ignore', message='.*TensorFlow.*')
        warnings.filterwarnings('ignore', message='.*Flax.*')
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # æœ¬åœ°tokenizerè·¯å¾„
        local_tokenizer_path = os.path.join(project_root, "deepseek_v3_tokenizer")
        
        if os.path.exists(local_tokenizer_path):
            try:
                logger.debug(f"[TOKEN_COUNTER] å°è¯•ä»æœ¬åœ°åŠ è½½DeepSeek tokenizer: {local_tokenizer_path}")
                _deepseek_tokenizer = AutoTokenizer.from_pretrained(
                    local_tokenizer_path,
                    local_files_only=True,
                    trust_remote_code=True
                )
                logger.info(f"[TOKEN_COUNTER] âœ… å·²ä»æœ¬åœ°åŠ è½½DeepSeek tokenizer")
                return _deepseek_tokenizer
            except Exception as e:
                logger.warning(f"[TOKEN_COUNTER] æœ¬åœ°åŠ è½½DeepSeek tokenizerå¤±è´¥: {e}")
        else:
            logger.info(f"[TOKEN_COUNTER] DeepSeek tokenizerç›®å½•ä¸å­˜åœ¨: {local_tokenizer_path}")
        
        # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
        try:
            logger.debug(f"[TOKEN_COUNTER] å°è¯•åœ¨çº¿ä¸‹è½½DeepSeek tokenizer...")
            _deepseek_tokenizer = AutoTokenizer.from_pretrained(
                "deepseek-ai/DeepSeek-V3",
                trust_remote_code=True,
                local_files_only=False
            )
            logger.info(f"[TOKEN_COUNTER] âœ… å·²åœ¨çº¿åŠ è½½DeepSeek tokenizer")
            return _deepseek_tokenizer
        except Exception as e:
            logger.debug(f"[TOKEN_COUNTER] DeepSeek tokenizeråœ¨çº¿ä¸‹è½½å¤±è´¥: {e}")
        
        logger.info("[TOKEN_COUNTER] DeepSeek tokenizerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨tiktokenä¼°ç®—")
        logger.info(f"[TOKEN_COUNTER] æç¤ºï¼šå¯å°†tokenizeræ–‡ä»¶æ”¾åˆ° {local_tokenizer_path} ç›®å½•")
        return None
        
    except ImportError:
        logger.debug("[TOKEN_COUNTER] transformersæœªå®‰è£…ï¼Œä½¿ç”¨tiktokenä½œä¸ºæ›¿ä»£")
        return None
    except Exception as e:
        logger.debug(f"[TOKEN_COUNTER] DeepSeek tokenizeråˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def get_tokenizer_for_model(model_name: str) -> str:
    """
    è·å–æ¨¡å‹åº”è¯¥ä½¿ç”¨çš„tokenizerç±»å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        tokenizerç±»å‹: 'anthropic', 'google', 'deepseek', 'tiktoken', æˆ– 'estimate'
    """
    config = load_tokenizer_config()
    model_lower = model_name.lower()
    
    # ç²¾ç¡®åŒ¹é…
    if model_lower in config:
        return config[model_lower]
    
    # æ¨¡ç³ŠåŒ¹é…
    for key, tokenizer_type in config.items():
        if key in model_lower:
            return tokenizer_type
    
    # é»˜è®¤ä½¿ç”¨tiktoken
    return 'tiktoken'

def get_anthropic_client():
    """
    è·å–Anthropicå®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºtokenè®¡æ•°ï¼‰
    
    Returns:
        Anthropicå®¢æˆ·ç«¯æˆ–None
    """
    global _anthropic_client
    
    if _anthropic_client is not None:
        return _anthropic_client
    
    try:
        import anthropic
        
        # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¸éœ€è¦API keyä¹Ÿèƒ½ä½¿ç”¨count_tokensï¼‰
        _anthropic_client = anthropic.Anthropic(api_key="dummy")
        logger.info("[TOKEN_COUNTER] å·²åŠ è½½Anthropic tokenizer")
        return _anthropic_client
        
    except ImportError:
        logger.debug("[TOKEN_COUNTER] anthropicæœªå®‰è£…ï¼Œè¿è¡Œ: pip install anthropic")
        return None
    except Exception as e:
        logger.warning(f"[TOKEN_COUNTER] åŠ è½½Anthropic tokenizerå¤±è´¥: {e}")
        return None

def get_gemma_tokenizer():
    """
    è·å–Gemma tokenizerå®ä¾‹ï¼ˆç”¨äºGemini tokenè®¡æ•°çš„æ›¿ä»£æ–¹æ¡ˆï¼‰
    ä¼˜å…ˆä»æœ¬åœ°tokenizersç›®å½•åŠ è½½
    
    Returns:
        Gemma tokenizerå®ä¾‹æˆ–None
    """
    global _gemma_tokenizer
    
    if _gemma_tokenizer is not None:
        return _gemma_tokenizer
    
    try:
        from transformers import AutoTokenizer
        import warnings
        
        # å¿½ç•¥PyTorch/TensorFlowæœªå®‰è£…çš„è­¦å‘Šï¼ˆtokenizerä¸éœ€è¦è¿™äº›ï¼‰
        warnings.filterwarnings('ignore', message='.*PyTorch.*')
        warnings.filterwarnings('ignore', message='.*TensorFlow.*')
        warnings.filterwarnings('ignore', message='.*Flax.*')
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # ä¼˜å…ˆå°è¯•æœ¬åœ°tokenizersç›®å½•
        local_tokenizer_paths = [
            os.path.join(project_root, "tokenizers", "gemma3-27b-it"),  # Gemma 3
            os.path.join(project_root, "tokenizers", "gemma-2b-it"),
            os.path.join(project_root, "tokenizers", "gemma-7b-it"),
            os.path.join(project_root, "tokenizers", "gemma-2b"),
            os.path.join(project_root, "tokenizers", "gemma"),  # é€šç”¨æ–‡ä»¶å¤¹å
        ]
        
        # å…ˆå°è¯•æœ¬åœ°è·¯å¾„
        for local_path in local_tokenizer_paths:
            if os.path.exists(local_path):
                try:
                    logger.debug(f"[TOKEN_COUNTER] å°è¯•ä»æœ¬åœ°åŠ è½½: {local_path}")
                    _gemma_tokenizer = AutoTokenizer.from_pretrained(
                        local_path,
                        local_files_only=True,
                        trust_remote_code=True
                    )
                    logger.info(f"[TOKEN_COUNTER] å·²ä»æœ¬åœ°åŠ è½½Gemma tokenizer: {os.path.basename(local_path)}")
                    return _gemma_tokenizer
                except Exception as e:
                    logger.debug(f"[TOKEN_COUNTER] æœ¬åœ°åŠ è½½å¤±è´¥ {local_path}: {e}")
                    continue
        
        # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
        online_options = [
            ("google/gemma-2b-it", "Gemma 2B IT"),
            ("google/gemma-7b-it", "Gemma 7B IT"),
        ]
        
        last_error = None
        for model_name, display_name in online_options:
            try:
                logger.debug(f"[TOKEN_COUNTER] å°è¯•åœ¨çº¿ä¸‹è½½ {display_name}...")
                _gemma_tokenizer = AutoTokenizer.from_pretrained(
                    model_name,
                    trust_remote_code=True,
                    local_files_only=False
                )
                logger.info(f"[TOKEN_COUNTER] å·²åŠ è½½Gemma tokenizer: {display_name}ï¼ˆç”¨äºGemini tokenè®¡æ•°ï¼‰")
                return _gemma_tokenizer
            except Exception as e:
                last_error = str(e)
                logger.debug(f"[TOKEN_COUNTER] {display_name} ä¸‹è½½å¤±è´¥: {type(e).__name__}")
                continue
        
        # æ‰€æœ‰é€‰é¡¹éƒ½å¤±è´¥
        logger.info("[TOKEN_COUNTER] Gemma tokenizerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨tiktokenï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼Œä¸å½±å“ä½¿ç”¨ï¼‰")
        logger.info(f"[TOKEN_COUNTER] æç¤ºï¼šå¯å°†tokenizeræ–‡ä»¶æ”¾åˆ° {os.path.join(project_root, 'tokenizers', 'gemma')} ç›®å½•")
        return None
        
    except ImportError:
        logger.debug("[TOKEN_COUNTER] transformersæœªå®‰è£…ï¼Œä½¿ç”¨tiktokenä½œä¸ºæ›¿ä»£")
        return None
    except Exception as e:
        logger.debug(f"[TOKEN_COUNTER] Gemma tokenizeråˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def get_gemini_model():
    """
    è·å–Geminiæ¨¡å‹å®ä¾‹ï¼ˆç”¨äºtokenè®¡æ•°ï¼‰
    
    Returns:
        Geminiæ¨¡å‹å®ä¾‹æˆ–None
    """
    global _gemini_model
    
    if _gemini_model is not None:
        return _gemini_model
    
    try:
        import google.generativeai as genai
        
        # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        api_key = os.environ.get('GOOGLE_API_KEY')
        if not api_key:
            # å°è¯•ä»configä¸­è·å–
            try:
                from core.config_loader import CONFIG
                api_key = CONFIG.get('google_api_key') or CONFIG.get('api_key')
            except:
                pass
        
        if not api_key:
            logger.debug("[TOKEN_COUNTER] Google APIå¯†é’¥æœªé…ç½®ï¼Œå°†ä½¿ç”¨Gemma tokenizerä½œä¸ºæ›¿ä»£")
            return None
        
        # é…ç½®APIå¯†é’¥
        genai.configure(api_key=api_key)
        
        # åˆ›å»ºä¸€ä¸ªç”¨äºtokenè®¡æ•°çš„æ¨¡å‹å®ä¾‹
        # ä½¿ç”¨gemini-proä½œä¸ºé»˜è®¤æ¨¡å‹
        _gemini_model = genai.GenerativeModel('gemini-pro')
        logger.info("[TOKEN_COUNTER] å·²åŠ è½½Google Gemini tokenizer")
        return _gemini_model
        
    except ImportError:
        logger.debug("[TOKEN_COUNTER] google-generativeaiæœªå®‰è£…ï¼Œå°†ä½¿ç”¨Gemma tokenizerä½œä¸ºæ›¿ä»£")
        return None
    except Exception as e:
        logger.debug(f"[TOKEN_COUNTER] Gemini tokenizerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨Gemma tokenizer: {e}")
        return None

def get_tiktoken_encoding(model_name: str):
    """
    è·å–tiktokenç¼–ç å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        tiktokenç¼–ç å™¨å®ä¾‹
    """
    try:
        import tiktoken
        
        # ç¼“å­˜tokenizerå®ä¾‹
        if model_name in _tiktoken_cache:
            return _tiktoken_cache[model_name]
        
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨
        encoding_name = None
        
        # GPT-4ç³»åˆ—
        if any(x in model_name.lower() for x in ['gpt-4', 'gpt4']):
            encoding_name = 'cl100k_base'
        # GPT-3.5ç³»åˆ—
        elif any(x in model_name.lower() for x in ['gpt-3.5', 'gpt3.5', 'turbo']):
            encoding_name = 'cl100k_base'
        # Claudeç³»åˆ—ä¹Ÿå¯ä»¥ç”¨cl100k_baseä½œä¸ºè¿‘ä¼¼
        elif 'claude' in model_name.lower():
            encoding_name = 'cl100k_base'
        # Geminiç³»åˆ—ä¹Ÿä½¿ç”¨cl100k_baseä½œä¸ºè¿‘ä¼¼
        elif 'gemini' in model_name.lower():
            encoding_name = 'cl100k_base'
        # é»˜è®¤ä½¿ç”¨cl100k_base
        else:
            encoding_name = 'cl100k_base'
        
        encoding = tiktoken.get_encoding(encoding_name)
        _tiktoken_cache[model_name] = encoding
        
        logger.info(f"[TOKEN_COUNTER] ä¸ºæ¨¡å‹ '{model_name}' åŠ è½½tokenizer: {encoding_name}")
        return encoding
        
    except ImportError:
        logger.warning("[TOKEN_COUNTER] tiktokenæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ä¼°ç®—æ–¹æ³•")
        return None
    except Exception as e:
        logger.error(f"[TOKEN_COUNTER] åŠ è½½tiktokenå¤±è´¥: {e}")
        return None

def count_text_tokens(text: str, model_name: str = "gpt-4") -> int:
    """
    è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆä¼˜å…ˆä½¿ç”¨åŸç”Ÿtokenizerï¼Œå¦åˆ™ä½¿ç”¨æ ¡å‡†ç³»æ•°ï¼‰
    
    Args:
        text: è¦è®¡ç®—çš„æ–‡æœ¬
        model_name: æ¨¡å‹åç§°
        
    Returns:
        tokenæ•°é‡
    """
    if not text:
        return 0
    
    # ğŸ”§ æ–°å¢ï¼šå¯¹äºDeepSeekæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨å®˜æ–¹tokenizer
    if 'deepseek' in model_name.lower():
        deepseek_tokenizer = get_deepseek_tokenizer()
        if deepseek_tokenizer:
            try:
                tokens = deepseek_tokenizer.encode(text)
                token_count = len(tokens)
                logger.info(f"[TOKEN_COUNTER] âœ… ä½¿ç”¨DeepSeekå®˜æ–¹tokenizerï¼ˆæ¨¡å‹: {model_name}ï¼‰: {token_count} tokens")
                return token_count
            except Exception as e:
                logger.warning(f"[TOKEN_COUNTER] DeepSeek tokenizerå¤±è´¥: {e}")
    
    # å¯¹äºGeminiæ¨¡å‹ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨å®˜æ–¹tokenizerï¼Œç„¶åæ˜¯Gemma tokenizer
    if 'gemini' in model_name.lower():
        # 1. ä¼˜å…ˆå°è¯•Googleå®˜æ–¹tokenizerï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
        gemini_model = get_gemini_model()
        if gemini_model:
            try:
                result = gemini_model.count_tokens(text)
                token_count = result.total_tokens
                logger.info(f"[TOKEN_COUNTER] âœ… ä½¿ç”¨Geminiå®˜æ–¹tokenizerï¼ˆæ¨¡å‹: {model_name}ï¼‰: {token_count} tokens")
                return token_count
            except Exception as e:
                logger.warning(f"[TOKEN_COUNTER] Geminiå®˜æ–¹tokenizerå¤±è´¥: {e}")
        
        # 2. å°è¯•ä½¿ç”¨Gemma tokenizerï¼ˆHugging Face transformersï¼‰
        gemma_tokenizer = get_gemma_tokenizer()
        if gemma_tokenizer:
            try:
                tokens = gemma_tokenizer.encode(text)
                token_count = len(tokens)
                logger.info(f"[TOKEN_COUNTER] âœ… ä½¿ç”¨Gemma tokenizerï¼ˆæ¨¡å‹: {model_name}ï¼‰: {token_count} tokens")
                return token_count
            except Exception as e:
                logger.warning(f"[TOKEN_COUNTER] Gemma tokenizerå¤±è´¥: {e}")
    
    # è·å–æ¨¡å‹çš„æ ¡å‡†ç³»æ•°
    multiplier = get_model_multiplier(model_name)
    
    # å°è¯•ä½¿ç”¨tiktoken
    encoding = get_tiktoken_encoding(model_name)
    if encoding:
        try:
            tokens = encoding.encode(text)
            base_count = len(tokens)
            # åº”ç”¨æ ¡å‡†ç³»æ•°
            adjusted_count = int(base_count * multiplier)
            
            if multiplier != 1.0:
                logger.info(f"[TOKEN_COUNTER] âœ… ä½¿ç”¨Tiktokenï¼ˆæ¨¡å‹: {model_name}, æ ¡å‡†ç³»æ•°{multiplier}ï¼‰: {base_count} -> {adjusted_count} tokens")
            else:
                logger.info(f"[TOKEN_COUNTER] âœ… ä½¿ç”¨Tiktokenï¼ˆæ¨¡å‹: {model_name}ï¼‰: {adjusted_count} tokens")
            
            return adjusted_count
        except Exception as e:
            logger.error(f"[TOKEN_COUNTER] tiktokenç¼–ç å¤±è´¥: {e}")
    
    # å›é€€åˆ°ä¼°ç®—ï¼ˆå­—ç¬¦æ•°Ã·4å¯¹è‹±æ–‡è¾ƒå‡†ï¼ŒÃ·2å¯¹ä¸­æ–‡è¾ƒå‡†ï¼‰
    # æ£€æµ‹æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    total_chars = len(text)
    
    if chinese_chars > total_chars * 0.5:
        # ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œä½¿ç”¨Ã·2
        base_estimate = total_chars // 2
    else:
        # ä¸»è¦æ˜¯è‹±æ–‡æˆ–æ··åˆï¼Œä½¿ç”¨Ã·4
        base_estimate = total_chars // 4
    
    # åº”ç”¨æ ¡å‡†ç³»æ•°
    return int(base_estimate * multiplier)

def count_messages_tokens(messages: List[Dict[str, Any]], model_name: str = "gpt-4") -> Tuple[int, Dict[str, int]]:
    """
    è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°é‡ï¼ˆåŒ…å«æ¶ˆæ¯æ ¼å¼çš„å¼€é”€ï¼Œè€ƒè™‘æ¨¡å‹æ ¡å‡†ç³»æ•°ï¼‰
    
    Args:
        messages: OpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        
    Returns:
        (æ€»tokenæ•°, è¯¦ç»†ç»Ÿè®¡å­—å…¸)
    """
    total_tokens = 0
    details = {
        'messages': 0,
        'system': 0,
        'user': 0,
        'assistant': 0,
        'overhead': 0,
        'multiplier': get_model_multiplier(model_name)
    }
    
    # æ¶ˆæ¯æ ¼å¼å¼€é”€ï¼ˆæ ¹æ®OpenAIçš„è®¡ç®—æ–¹å¼ï¼‰
    # æ¯æ¡æ¶ˆæ¯ï¼š<|start|>role\ncontent<|end|>\n = çº¦4ä¸ªtoken
    # æ•´ä¸ªå¯¹è¯ï¼š<|start|>assistant<|message|> = çº¦3ä¸ªtoken
    
    for message in messages:
        role = message.get('role', 'user')
        content = message.get('content', '')
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
        text_content = ''
        if isinstance(content, str):
            text_content = content
        elif isinstance(content, list):
            # æå–æ–‡æœ¬éƒ¨åˆ†
            for part in content:
                if isinstance(part, dict) and part.get('type') == 'text':
                    text_content += part.get('text', '')
        
        # è®¡ç®—å†…å®¹tokenæ•°ï¼ˆå·²åŒ…å«æ ¡å‡†ç³»æ•°ï¼‰
        content_tokens = count_text_tokens(text_content, model_name)
        
        # æ·»åŠ æ¶ˆæ¯æ ¼å¼å¼€é”€ï¼ˆçº¦4ä¸ªtokenæ¯æ¡æ¶ˆæ¯ï¼Œä¹Ÿéœ€è¦åº”ç”¨æ ¡å‡†ç³»æ•°ï¼‰
        overhead_per_message = int(4 * details['multiplier'])
        message_tokens = content_tokens + overhead_per_message
        
        total_tokens += message_tokens
        details['messages'] += message_tokens
        details[role] = details.get(role, 0) + content_tokens
    
    # æ·»åŠ æ•´ä½“å¯¹è¯å¼€é”€ï¼ˆä¹Ÿåº”ç”¨æ ¡å‡†ç³»æ•°ï¼‰
    overall_overhead = int((len(messages) * 4 + 3) * details['multiplier'])
    details['overhead'] = overall_overhead
    total_tokens += overall_overhead
    
    return total_tokens, details

def count_response_tokens(response_text: str, model_name: str = "gpt-4") -> int:
    """
    è®¡ç®—å“åº”æ–‡æœ¬çš„tokenæ•°é‡
    
    Args:
        response_text: å“åº”æ–‡æœ¬
        model_name: æ¨¡å‹åç§°
        
    Returns:
        tokenæ•°é‡
    """
    return count_text_tokens(response_text, model_name)

def get_token_counter_info() -> Dict[str, Any]:
    """
    è·å–tokenè®¡æ•°å™¨ä¿¡æ¯
    
    Returns:
        è®¡æ•°å™¨ä¿¡æ¯å­—å…¸
    """
    info = {
        'tiktoken_available': False,
        'cached_models': list(_tiktoken_cache.keys()),
        'method': 'estimation'
    }
    
    try:
        import tiktoken
        info['tiktoken_available'] = True
        info['tiktoken_version'] = tiktoken.__version__ if hasattr(tiktoken, '__version__') else 'unknown'
        info['method'] = 'tiktoken'
    except ImportError:
        pass
    
    return info

# å¯¼å‡ºçš„ä¾¿æ·å‡½æ•°
def estimate_tokens(text: str, model: str = "gpt-4") -> int:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¼°ç®—æ–‡æœ¬tokenæ•°
    
    Args:
        text: æ–‡æœ¬å†…å®¹
        model: æ¨¡å‹åç§°
        
    Returns:
        tokenæ•°é‡
    """
    return count_text_tokens(text, model)

def estimate_message_tokens(messages: List[Dict], model: str = "gpt-4") -> int:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¼°ç®—æ¶ˆæ¯tokenæ•°
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        model: æ¨¡å‹åç§°
        
    Returns:
        æ€»tokenæ•°
    """
    total, _ = count_messages_tokens(messages, model)
    return total