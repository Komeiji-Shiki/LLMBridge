"""
ç›‘æ§æ¨¡å— - ç”¨äºæ”¶é›†å’Œç®¡ç†è¯·æ±‚ç»Ÿè®¡æ•°æ®
æ–°ç‰ˆæœ¬ï¼šåˆ†å±‚æ—¥å¿—å­˜å‚¨ç³»ç»Ÿ
- æŒ‰æ—¥æœŸï¼ˆå¤©ï¼‰åˆ†æ–‡ä»¶å¤¹
- æŒ‰å°æ—¶åˆ†å­æ–‡ä»¶å¤¹
- æ¯ä¸ªè¯·æ±‚ä¸€ä¸ªç‹¬ç«‹çš„JSONæ–‡ä»¶
"""

import json
import time
import threading
import gzip
from datetime import datetime, timedelta
from collections import defaultdict, deque
from dataclasses import dataclass, asdict
from typing import Dict, Optional, List
import logging
from pathlib import Path

# å¯¼å…¥SQLiteæ‰©å±•
try:
    from modules.monitoring_sqlite import SQLiteLogger
    SQLITE_AVAILABLE = True
except ImportError as e:
    logger.warning(f"SQLiteæ‰©å±•ä¸å¯ç”¨: {e}")
    SQLITE_AVAILABLE = False
logger = logging.getLogger(__name__)

# é…ç½®
class MonitorConfig:
    """ç›‘æ§é…ç½®"""
    LOG_DIR = Path("logs")
    
    # SQLiteæ•°æ®åº“é…ç½®
    DB_FILE = "requests.db"
    ENABLE_SQLITE = True  # æ˜¯å¦å¯ç”¨SQLiteæ•°æ®åº“ï¼ˆé«˜æ€§èƒ½æŸ¥è¯¢ï¼‰
    # æ—§ç‰ˆæœ¬çš„JSONLæ–‡ä»¶ï¼ˆä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼‰
    REQUEST_LOG_FILE = "requests.jsonl"
    ERROR_LOG_FILE = "errors.jsonl"
    STATS_FILE = "stats.json"
    
    # æ–°ç‰ˆæœ¬ï¼šåˆ†å±‚æ—¥å¿—é…ç½®
    ENABLE_HIERARCHICAL_LOGS = True  # æ˜¯å¦å¯ç”¨æ–°çš„åˆ†å±‚æ—¥å¿—ç³»ç»Ÿ
    ENABLE_LEGACY_LOGS = False  # ğŸ”§ ç¦ç”¨JSONLæ—¥å¿—ï¼ˆå·²ä½¿ç”¨SQLiteå’Œåˆ†å±‚JSONï¼‰
    USE_COMPRESSION = False  # æ˜¯å¦ä½¿ç”¨gzipå‹ç¼©ï¼ˆ.json.gzï¼‰
    
    # æ—¥å¿—ä¿ç•™ç­–ç•¥
    MAX_LOG_DAYS = 30  # ä¿ç•™æœ€è¿‘Nå¤©çš„æ—¥å¿—
    MAX_LOGS_PER_HOUR = 10000  # æ¯å°æ—¶æœ€å¤šä¿ç•™çš„æ—¥å¿—æ–‡ä»¶æ•°
    
    # å…¶ä»–é…ç½®
    MAX_LOG_SIZE = 400 * 1024 * 1024  # å•æ–‡ä»¶æœ€å¤§å¤§å°ï¼ˆä»…ç”¨äºæ—§JSONLï¼‰
    MAX_LOG_FILES = 10  # æ—§JSONLæ–‡ä»¶çš„è½®è½¬æ•°é‡
    MAX_RECENT_REQUESTS = 10000
    MAX_RECENT_ERRORS = 50
    STATS_UPDATE_INTERVAL = 5  # ç§’

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
MonitorConfig.LOG_DIR.mkdir(exist_ok=True)

@dataclass
class RequestInfo:
    """è¯·æ±‚ä¿¡æ¯"""
    request_id: str
    timestamp: float
    model: str
    status: str  # 'active', 'success', 'failed'
    duration: Optional[float] = None
    error: Optional[str] = None
    messages_count: int = 0
    session_id: Optional[str] = None
    mode: Optional[str] = None
    # æ–°å¢è¯¦ç»†ä¿¡æ¯å­—æ®µ
    request_messages: Optional[List[dict]] = None
    request_params: Optional[dict] = None
    response_content: Optional[str] = None
    reasoning_content: Optional[str] = None  # æ–°å¢ï¼šæ€ç»´é“¾å†…å®¹
    input_tokens: int = 0
    output_tokens: int = 0

@dataclass
class Stats:
    """ç»Ÿè®¡æ•°æ®"""
    total_requests: int = 0
    success_requests: int = 0  # ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨success_requests
    failed_requests: int = 0
    active_requests: int = 0
    avg_duration: float = 0.0
    total_messages: int = 0
    uptime: float = 0.0

class LogManager:
    """æ—¥å¿—ç®¡ç†å™¨ - æ”¯æŒæ–°æ—§ä¸¤ç§æ—¥å¿—æ ¼å¼"""
    
    def __init__(self):
        self.request_log_path = MonitorConfig.LOG_DIR / MonitorConfig.REQUEST_LOG_FILE
        self.error_log_path = MonitorConfig.LOG_DIR / MonitorConfig.ERROR_LOG_FILE
        self._lock = threading.Lock()
        self._counter_lock = threading.Lock()
        self._hourly_counters = {}  # {(date, hour): counter} ç”¨äºç”Ÿæˆåºå·
        
        # åˆå§‹åŒ–SQLiteæ—¥å¿—å™¨
        self.sqlite_logger = None
        if MonitorConfig.ENABLE_SQLITE and SQLITE_AVAILABLE:
            try:
                db_path = MonitorConfig.LOG_DIR / MonitorConfig.DB_FILE
                self.sqlite_logger = SQLiteLogger(db_path)
                logger.info("âœ… SQLiteæ—¥å¿—å™¨å·²å¯ç”¨")
            except Exception as e:
                logger.error(f"åˆå§‹åŒ–SQLiteæ—¥å¿—å™¨å¤±è´¥: {e}")
        
    def _get_hierarchical_log_path(self, timestamp: float, request_id: str, log_type: str = "request", model_name: str = None) -> Path:
        """
        ç”Ÿæˆåˆ†å±‚æ—¥å¿—æ–‡ä»¶è·¯å¾„
        æ ¼å¼: logs/YYYYMMDD/HH/æ¨¡å‹å_YYYYMMDD_HHMM_requestID[:8].json[.gz]
        
        Args:
            timestamp: Unixæ—¶é—´æˆ³
            request_id: è¯·æ±‚ID
            log_type: æ—¥å¿—ç±»å‹ ("request" æˆ– "error")
            model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Pathå¯¹è±¡ï¼ŒæŒ‡å‘æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        dt = datetime.fromtimestamp(timestamp)
        date_str = dt.strftime("%Y%m%d")  # æ—¥æœŸæ–‡ä»¶å¤¹
        hour_str = dt.strftime("%H")      # å°æ—¶æ–‡ä»¶å¤¹
        datetime_str = dt.strftime("%Y%m%d_%H%M")  # ç²¾ç¡®åˆ°åˆ†é’Ÿçš„æ—¥æœŸæ—¶é—´
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        date_dir = MonitorConfig.LOG_DIR / date_str
        hour_dir = date_dir / hour_str
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        hour_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶åæ ¼å¼: æ¨¡å‹å_æ—¥æœŸæ—¶é—´_è¯·æ±‚IDå‰8ä½.json[.gz]
        req_id_short = request_id[:8] if request_id else "unknown"
        
        # å¤„ç†æ¨¡å‹åç§°ï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…æ–‡ä»¶åé—®é¢˜ï¼‰
        if model_name:
            # æ›¿æ¢ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦
            safe_model_name = model_name.replace('/', '-').replace('\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('"', '-').replace('<', '-').replace('>', '-').replace('|', '-')
            # é™åˆ¶é•¿åº¦
            if len(safe_model_name) > 50:
                safe_model_name = safe_model_name[:50]
        else:
            safe_model_name = "unknown"
        
        file_ext = ".json.gz" if MonitorConfig.USE_COMPRESSION else ".json"
        filename = f"{safe_model_name}_{datetime_str}_{req_id_short}{file_ext}"
        
        return hour_dir / filename
    
    def _write_hierarchical_log(self, log_entry: dict, log_type: str = "request"):
        """
        å†™å…¥åˆ†å±‚æ—¥å¿—æ–‡ä»¶
        
        Args:
            log_entry: æ—¥å¿—æ¡ç›®å­—å…¸
            log_type: æ—¥å¿—ç±»å‹ ("request" æˆ– "error")
        """
        try:
            timestamp = log_entry.get('timestamp', time.time())
            request_id = log_entry.get('request_id', 'unknown')
            model_name = log_entry.get('model', 'unknown')
            
            file_path = self._get_hierarchical_log_path(timestamp, request_id, log_type, model_name)
            
            # å†™å…¥æ–‡ä»¶
            json_data = json.dumps(log_entry, ensure_ascii=False, indent=2)
            
            if MonitorConfig.USE_COMPRESSION:
                with gzip.open(file_path, 'wt', encoding='utf-8') as f:
                    f.write(json_data)
            else:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(json_data)
            
            logger.debug(f"å·²å†™å…¥åˆ†å±‚æ—¥å¿—: {file_path}")
            
        except Exception as e:
            logger.error(f"å†™å…¥åˆ†å±‚æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
    
    def write_request_log(self, log_entry: dict):
        """å†™å…¥è¯·æ±‚æ—¥å¿—ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼+SQLiteï¼‰"""
        with self._lock:
            try:
                # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šä¼˜å…ˆå†™å…¥SQLiteæ•°æ®åº“ï¼ˆå®æ—¶æ›´æ–°ï¼‰
                if self.sqlite_logger:
                    try:
                        self.sqlite_logger.write_request(log_entry)
                    except Exception as e:
                        logger.error(f"å†™å…¥SQLiteå¤±è´¥: {e}")
                
                # æ–°æ ¼å¼ï¼šåˆ†å±‚æ—¥å¿—
                if MonitorConfig.ENABLE_HIERARCHICAL_LOGS:
                    self._write_hierarchical_log(log_entry, log_type="request")
                
                # æ—§æ ¼å¼ï¼šJSONLï¼ˆå¯é€‰ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰
                if MonitorConfig.ENABLE_LEGACY_LOGS:
                    with open(self.request_log_path, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            except Exception as e:
                logger.error(f"å†™å…¥è¯·æ±‚æ—¥å¿—å¤±è´¥: {e}")
    
    def write_error_log(self, log_entry: dict):
        """å†™å…¥é”™è¯¯æ—¥å¿—ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼ï¼‰"""
        with self._lock:
            try:
                # æ–°æ ¼å¼ï¼šåˆ†å±‚æ—¥å¿—
                if MonitorConfig.ENABLE_HIERARCHICAL_LOGS:
                    self._write_hierarchical_log(log_entry, log_type="error")
                
                # æ—§æ ¼å¼ï¼šJSONLï¼ˆå¯é€‰ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰
                if MonitorConfig.ENABLE_LEGACY_LOGS:
                    with open(self.error_log_path, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
            except Exception as e:
                logger.error(f"å†™å…¥é”™è¯¯æ—¥å¿—å¤±è´¥: {e}")
    
    def _read_hierarchical_logs(self, log_type: str = "request", limit: int = 50,
                                days_back: int = 7) -> List[dict]:
        """
        ä»åˆ†å±‚æ—¥å¿—ä¸­è¯»å–æœ€è¿‘çš„æ—¥å¿—
        
        Args:
            log_type: æ—¥å¿—ç±»å‹ ("request" æˆ– "error")
            limit: è¿”å›çš„æœ€å¤§æ—¥å¿—æ•°é‡
            days_back: å‘å‰æœç´¢çš„å¤©æ•°
        
        Returns:
            æ—¥å¿—æ¡ç›®åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
        """
        logs = []
        
        try:
            # è·å–æœ€è¿‘Nå¤©çš„æ—¥æœŸåˆ—è¡¨
            today = datetime.now()
            dates_to_check = []
            for i in range(days_back):
                date = today - timedelta(days=i)
                date_str = date.strftime("%Y%m%d")
                dates_to_check.append(date_str)
            
            # æ”¶é›†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´å€’åºï¼‰
            all_log_files = []
            for date_str in dates_to_check:
                date_dir = MonitorConfig.LOG_DIR / date_str
                if not date_dir.exists():
                    continue
                
                # éå†è¯¥æ—¥æœŸä¸‹çš„æ‰€æœ‰å°æ—¶æ–‡ä»¶å¤¹
                for hour_dir in sorted(date_dir.iterdir(), reverse=True):
                    if not hour_dir.is_dir():
                        continue
                    
                    # è·å–è¯¥å°æ—¶ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
                    pattern = "*.json.gz" if MonitorConfig.USE_COMPRESSION else "*.json"
                    for log_file in sorted(hour_dir.glob(pattern), reverse=True):
                        all_log_files.append(log_file)
                        
                        # æå‰é€€å‡ºä¼˜åŒ–ï¼šå¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿå¤šçš„æ–‡ä»¶
                        if len(all_log_files) >= limit * 2:
                            break
                    
                    if len(all_log_files) >= limit * 2:
                        break
                
                if len(all_log_files) >= limit * 2:
                    break
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            for log_file in all_log_files:
                if len(logs) >= limit:
                    break
                
                try:
                    if MonitorConfig.USE_COMPRESSION:
                        with gzip.open(log_file, 'rt', encoding='utf-8') as f:
                            log_entry = json.load(f)
                    else:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            log_entry = json.load(f)
                    
                    # è¿‡æ»¤æ—¥å¿—ç±»å‹
                    if log_type == "request" and log_entry.get('type') == 'request_end':
                        logs.append(log_entry)
                    elif log_type == "error":
                        logs.append(log_entry)
                
                except Exception as e:
                    logger.warning(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
                    continue
            
            return logs
            
        except Exception as e:
            logger.error(f"è¯»å–åˆ†å±‚æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
            return []
    
    def read_recent_logs(self, log_type: str = "requests", limit: int = 50) -> List[dict]:
        """è¯»å–æœ€è¿‘çš„æ—¥å¿—ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨æ–°æ ¼å¼ï¼‰"""
        # å¦‚æœå¯ç”¨äº†åˆ†å±‚æ—¥å¿—ï¼Œä»åˆ†å±‚æ—¥å¿—è¯»å–
        if MonitorConfig.ENABLE_HIERARCHICAL_LOGS:
            log_type_internal = "request" if log_type == "requests" else "error"
            return self._read_hierarchical_logs(log_type_internal, limit)
        
        # å¦åˆ™ä»æ—§çš„JSONLæ–‡ä»¶è¯»å–
        log_path = self.request_log_path if log_type == "requests" else self.error_log_path
        logs = []
        
        if not log_path.exists():
            return logs
            
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # ä»åå¾€å‰è¯»å–ï¼Œæ”¶é›†æœ€è¿‘çš„ request_end ç±»å‹æ—¥å¿—
                for line in reversed(lines):
                    if len(logs) >= limit:
                        break
                    try:
                        log_entry = json.loads(line.strip())
                        # åªè¿”å› request_end ç±»å‹çš„æ—¥å¿—ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
                        if log_type == "requests" and log_entry.get('type') == 'request_end':
                            logs.append(log_entry)
                        elif log_type == "errors":
                            # é”™è¯¯æ—¥å¿—ä¸éœ€è¦è¿‡æ»¤
                            logs.append(log_entry)
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}")
            
        return logs  # å·²ç»æ˜¯å€’åºçš„ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰

class MonitoringService:
    """ç›‘æ§æœåŠ¡"""
    
    def __init__(self):
        self.startup_time = time.time()
        self.log_manager = LogManager()
        self.active_requests: Dict[str, RequestInfo] = {}
        self.recent_requests = deque(maxlen=MonitorConfig.MAX_RECENT_REQUESTS)
        self.recent_errors = deque(maxlen=MonitorConfig.MAX_RECENT_ERRORS)
        self.model_stats = defaultdict(lambda: {
            'total': 0, 'success': 0, 'failed': 0,
            'total_duration': 0, 'count_with_duration': 0
        })
        self._lock = threading.Lock()
        
        # æ–°å¢ï¼šå­˜å‚¨å®Œæ•´çš„è¯·æ±‚è¯¦æƒ…ï¼ˆç”¨äºè¯¦æƒ…æŸ¥çœ‹ï¼‰
        # ä½¿ç”¨OrderedDictå®ç°æ›´å¥½çš„å†…å­˜ç®¡ç†
        from collections import OrderedDict
        self.request_details_cache = OrderedDict()  # ä½¿ç”¨OrderedDictç®¡ç†ç¼“å­˜
        self.MAX_DETAILS_CACHE = 10000  # ä¿æŒåŸæœ‰çš„ç¼“å­˜å¤§å°
        self.cache_size_limit_mb = 500  # å¢åŠ ç¼“å­˜å¤§å°é™åˆ¶ä¸º500MBï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
        
        # WebSocketå®¢æˆ·ç«¯ç®¡ç†
        self.monitor_clients = set()
        
        # ğŸ”§ æ–°å¢ï¼šæ´»è·ƒè¯·æ±‚è¶…æ—¶é…ç½®ï¼ˆé»˜è®¤10åˆ†é’Ÿï¼‰
        self.active_request_timeout = 600  # 10åˆ†é’Ÿï¼Œè¶…è¿‡æ­¤æ—¶é—´çš„æ´»è·ƒè¯·æ±‚å°†è¢«è‡ªåŠ¨æ¸…ç†
        
        # åŠ è½½æŒä¹…åŒ–çš„ç»Ÿè®¡æ•°æ®
        self._load_persisted_stats()
        
        logger.info("ç›‘æ§æœåŠ¡å·²åˆå§‹åŒ–")
    
    def request_start(self, request_id: str, model: str, messages_count: int = 0,
                     session_id: str = None, mode: str = None,
                     messages: List[dict] = None, params: dict = None):
        """è®°å½•è¯·æ±‚å¼€å§‹ï¼ˆå¢åŠ è¯¦ç»†ä¿¡æ¯ï¼‰"""
        with self._lock:
            # è®¡ç®—è¾“å…¥tokençš„ä¼°ç®—å€¼
            estimated_input_tokens = 0
            if messages:
                for msg in messages:
                    if isinstance(msg, dict) and 'content' in msg:
                        content = msg.get('content', '')
                        if isinstance(content, str):
                            estimated_input_tokens += len(content) // 4
                        elif isinstance(content, list):
                            # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯
                            for part in content:
                                if isinstance(part, dict) and part.get('type') == 'text':
                                    estimated_input_tokens += len(part.get('text', '')) // 4
            
            request_info = RequestInfo(
                request_id=request_id,
                timestamp=time.time(),
                model=model,
                status='active',
                messages_count=messages_count,
                session_id=session_id,
                mode=mode,
                request_messages=messages,
                request_params=params,
                input_tokens=estimated_input_tokens  # è®¾ç½®ä¼°ç®—çš„è¾“å…¥token
            )
            self.active_requests[request_id] = request_info
            
            # åŒæ—¶å­˜å‚¨åˆ°è¯¦æƒ…ç¼“å­˜
            self._store_request_details(request_id, request_info)
            
            # å†™å…¥æ—¥å¿—
            log_entry = {
                'type': 'request_start',
                'timestamp': request_info.timestamp,
                'request_id': request_id,
                'model': model,
                'messages_count': messages_count,
                'session_id': session_id,
                'mode': mode
            }
            self.log_manager.write_request_log(log_entry)
            
            logger.info(f"è¯·æ±‚å¼€å§‹ [ID: {request_id[:8]}] æ¨¡å‹: {model}")
    
    def request_end(self, request_id: str, success: bool, error: str = None,
                    response_content: str = None, reasoning_content: str = None,
                    input_tokens: int = 0, output_tokens: int = 0, cost_info: dict = None):
        """è®°å½•è¯·æ±‚ç»“æŸï¼ˆå¢åŠ å“åº”å†…å®¹ã€æ€ç»´é“¾å’Œæˆæœ¬ä¿¡æ¯ï¼‰"""
        with self._lock:
            if request_id not in self.active_requests:
                logger.warning(f"æœªæ‰¾åˆ°è¯·æ±‚ {request_id}")
                return
                
            request_info = self.active_requests[request_id]
            request_info.status = 'success' if success else 'failed'
            request_info.duration = time.time() - request_info.timestamp
            request_info.error = error
            request_info.response_content = response_content
            request_info.reasoning_content = reasoning_content
            request_info.input_tokens = input_tokens
            request_info.output_tokens = output_tokens
            
            # æ›´æ–°è¯¦æƒ…ç¼“å­˜
            self._store_request_details(request_id, request_info)
            
            # æ›´æ–°æ¨¡å‹ç»Ÿè®¡
            model = request_info.model
            self.model_stats[model]['total'] += 1
            if success:
                self.model_stats[model]['success'] += 1
            else:
                self.model_stats[model]['failed'] += 1
                
            if request_info.duration:
                self.model_stats[model]['total_duration'] += request_info.duration
                self.model_stats[model]['count_with_duration'] += 1
            
            # æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®
            self._persist_stats()
            
            # æ·»åŠ åˆ°æœ€è¿‘è¯·æ±‚åˆ—è¡¨
            self.recent_requests.append(asdict(request_info))
            
            # å¦‚æœå¤±è´¥ï¼Œæ·»åŠ åˆ°é”™è¯¯åˆ—è¡¨
            if not success:
                error_info = {
                    'timestamp': time.time(),
                    'request_id': request_id,
                    'model': model,
                    'error': error or 'Unknown error'
                }
                self.recent_errors.append(error_info)
                self.log_manager.write_error_log(error_info)
            
            # å†™å…¥è¯·æ±‚æ—¥å¿—ï¼ˆåŒ…å«å®Œæ•´è¯¦æƒ…å’Œæˆæœ¬ä¿¡æ¯ï¼‰
            log_entry = {
                'type': 'request_end',
                'timestamp': time.time(),
                'request_id': request_id,
                'model': model,
                'status': request_info.status,
                'success': success,  # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ successå¸ƒå°”å­—æ®µ
                'duration': request_info.duration,
                'error': error,
                'mode': request_info.mode,
                'session_id': request_info.session_id,
                'messages_count': request_info.messages_count,
                'input_tokens': request_info.input_tokens,
                'output_tokens': request_info.output_tokens,
                # åŒ…å«è¯¦ç»†ä¿¡æ¯
                'request_messages': request_info.request_messages,
                'request_params': request_info.request_params,
                'response_content': request_info.response_content,
                'reasoning_content': request_info.reasoning_content,
                # ğŸ”§ æ–°å¢ï¼šæˆæœ¬ä¿¡æ¯
                'cost_info': cost_info
            }
            self.log_manager.write_request_log(log_entry)
            
            # ä»æ´»åŠ¨è¯·æ±‚ä¸­ç§»é™¤
            del self.active_requests[request_id]
            
            logger.info(f"è¯·æ±‚ç»“æŸ [ID: {request_id[:8]}] çŠ¶æ€: {request_info.status} è€—æ—¶: {request_info.duration:.2f}s")
    
    def get_stats(self) -> Stats:
        """è·å–ç»Ÿè®¡æ•°æ®"""
        with self._lock:
            stats = Stats()
            stats.uptime = time.time() - self.startup_time
            stats.active_requests = len(self.active_requests)
            
            # è·å–æ‰€æœ‰æ—¶é—´çš„ç»Ÿè®¡ï¼ˆä»æŒä¹…åŒ–æ•°æ®ï¼‰
            # ä¼˜å…ˆä½¿ç”¨æŒä¹…åŒ–çš„æ€»æ•°ï¼Œè¿™æ ·å³ä½¿é‡å¯æœåŠ¡å™¨ä¹Ÿèƒ½ä¿æŒå‡†ç¡®
            total_all_time = sum(s['total'] for s in self.model_stats.values())
            success_all_time = sum(s['success'] for s in self.model_stats.values())
            failed_all_time = sum(s['failed'] for s in self.model_stats.values())
            
            # ä½¿ç”¨æ‰€æœ‰æ—¶é—´çš„æ€»æ•°
            stats.total_requests = total_all_time
            stats.success_requests = success_all_time  # ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨success_requests
            stats.failed_requests = failed_all_time
            
            # è®¡ç®—æ€»æ¶ˆæ¯æ•°ï¼ˆä»æœ€è¿‘çš„è¯·æ±‚ä¸­ç´¯åŠ ï¼‰
            stats.total_messages = sum(req.get('messages_count', 0) for req in self.recent_requests)
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´ï¼ˆä½¿ç”¨æœ€è¿‘100ä¸ªè¯·æ±‚ï¼‰
            recent_durations = []
            for req in list(self.recent_requests)[-100:]:  # æœ€è¿‘100ä¸ªè¯·æ±‚
                if req.get('duration'):
                    recent_durations.append(req['duration'])
            
            if recent_durations:
                stats.avg_duration = sum(recent_durations) / len(recent_durations)
                
            return stats
    
    def get_model_stats(self) -> List[dict]:
        """è·å–æ¨¡å‹ç»Ÿè®¡"""
        with self._lock:
            model_stats_list = []
            for model, stats in self.model_stats.items():
                avg_duration = 0
                if stats['count_with_duration'] > 0:
                    avg_duration = stats['total_duration'] / stats['count_with_duration']
                    
                success_rate = 0
                if stats['total'] > 0:
                    success_rate = (stats['success'] / stats['total']) * 100
                    
                model_stats_list.append({
                    'model': model,
                    'total_requests': stats['total'],
                    'success_requests': stats['success'],  # ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨success_requests
                    'failed_requests': stats['failed'],
                    'avg_duration': avg_duration,
                    'success_rate': success_rate
                })
            
            # æŒ‰æ€»è¯·æ±‚æ•°æ’åº
            model_stats_list.sort(key=lambda x: x['total_requests'], reverse=True)
            return model_stats_list
    
    def get_active_requests(self) -> List[dict]:
        """è·å–æ´»åŠ¨è¯·æ±‚åˆ—è¡¨"""
        with self._lock:
            return [asdict(req) for req in self.active_requests.values()]
    
    def cleanup_stale_requests(self) -> int:
        """
        ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šæ¸…ç†è¶…æ—¶çš„æ´»è·ƒè¯·æ±‚
        
        Returns:
            æ¸…ç†çš„è¯·æ±‚æ•°é‡
        """
        with self._lock:
            current_time = time.time()
            stale_requests = []
            
            # æŸ¥æ‰¾è¶…æ—¶çš„è¯·æ±‚
            for request_id, request_info in self.active_requests.items():
                request_age = current_time - request_info.timestamp
                if request_age > self.active_request_timeout:
                    stale_requests.append(request_id)
                    logger.warning(f"[CLEANUP] å‘ç°è¶…æ—¶æ´»è·ƒè¯·æ±‚: {request_id[:8]} (å­˜æ´»: {request_age:.1f}ç§’)")
            
            # æ¸…ç†è¶…æ—¶çš„è¯·æ±‚
            for request_id in stale_requests:
                request_info = self.active_requests[request_id]
                
                # æ ‡è®°ä¸ºå¤±è´¥å¹¶è®°å½•
                request_info.status = 'failed'
                request_info.duration = current_time - request_info.timestamp
                request_info.error = f"Request timeout after {request_info.duration:.1f} seconds"
                
                # æ›´æ–°ç»Ÿè®¡
                model = request_info.model
                self.model_stats[model]['total'] += 1
                self.model_stats[model]['failed'] += 1
                
                # æ·»åŠ åˆ°é”™è¯¯åˆ—è¡¨
                error_info = {
                    'timestamp': current_time,
                    'request_id': request_id,
                    'model': model,
                    'error': request_info.error
                }
                self.recent_errors.append(error_info)
                self.log_manager.write_error_log(error_info)
                
                # å†™å…¥è¯·æ±‚æ—¥å¿—
                log_entry = {
                    'type': 'request_end',
                    'timestamp': current_time,
                    'request_id': request_id,
                    'model': model,
                    'status': 'failed',
                    'success': False,  # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ successå­—æ®µ
                    'duration': request_info.duration,
                    'error': request_info.error,
                    'mode': request_info.mode,
                    'session_id': request_info.session_id,
                    'messages_count': request_info.messages_count
                }
                self.log_manager.write_request_log(log_entry)
                
                # æ·»åŠ åˆ°æœ€è¿‘è¯·æ±‚åˆ—è¡¨
                self.recent_requests.append(asdict(request_info))
                
                # ä»æ´»åŠ¨è¯·æ±‚ä¸­ç§»é™¤
                del self.active_requests[request_id]
                
                logger.info(f"[CLEANUP] å·²æ¸…ç†è¶…æ—¶è¯·æ±‚: {request_id[:8]} (è¶…æ—¶: {request_info.duration:.1f}ç§’)")
            
            if stale_requests:
                # æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®
                self._persist_stats()
                logger.warning(f"[CLEANUP] å…±æ¸…ç†äº† {len(stale_requests)} ä¸ªè¶…æ—¶æ´»è·ƒè¯·æ±‚")
            
            return len(stale_requests)
    
    def get_recent_requests(self, limit: int = 50) -> List[dict]:
        """è·å–æœ€è¿‘çš„è¯·æ±‚"""
        with self._lock:
            requests = list(self.recent_requests)
            return requests[-limit:][::-1]  # æœ€æ–°çš„åœ¨å‰
    
    def get_recent_errors(self, limit: int = 30) -> List[dict]:
        """è·å–æœ€è¿‘çš„é”™è¯¯"""
        with self._lock:
            errors = list(self.recent_errors)
            return errors[-limit:][::-1]  # æœ€æ–°çš„åœ¨å‰
    
    def get_summary(self) -> dict:
        """è·å–ç›‘æ§æ‘˜è¦"""
        stats = self.get_stats()
        model_stats = self.get_model_stats()
        
        return {
            'stats': asdict(stats),
            'model_stats': model_stats,
            'active_requests_list': self.get_active_requests(),
            'recent_errors_count': len(self.recent_errors)
        }
    
    async def broadcast_to_monitors(self, data: dict):
        """å‘æ‰€æœ‰ç›‘æ§å®¢æˆ·ç«¯å¹¿æ’­æ•°æ®"""
        if not self.monitor_clients:
            return
            
        disconnected = []
        for client in self.monitor_clients:
            try:
                await client.send_json(data)
            except:
                disconnected.append(client)
        
        # æ¸…ç†æ–­å¼€çš„è¿æ¥
        for client in disconnected:
            self.monitor_clients.discard(client)
    
    def add_monitor_client(self, websocket):
        """æ·»åŠ ç›‘æ§å®¢æˆ·ç«¯"""
        self.monitor_clients.add(websocket)
        logger.debug(f"ç›‘æ§å®¢æˆ·ç«¯å·²è¿æ¥ï¼Œå½“å‰å®¢æˆ·ç«¯æ•°: {len(self.monitor_clients)}")
    
    def remove_monitor_client(self, websocket):
        """ç§»é™¤ç›‘æ§å®¢æˆ·ç«¯"""
        self.monitor_clients.discard(websocket)
        logger.debug(f"ç›‘æ§å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œå½“å‰å®¢æˆ·ç«¯æ•°: {len(self.monitor_clients)}")
    
    def _store_request_details(self, request_id: str, request_info: RequestInfo):
        """å­˜å‚¨è¯·æ±‚è¯¦æƒ…åˆ°ç¼“å­˜ï¼ˆä¿æŒæ•°æ®å®Œæ•´æ€§ï¼‰"""
        import sys
        
        # åˆ›å»ºè¦å­˜å‚¨çš„æ•°æ® - ä¿æŒå®Œæ•´æ€§ï¼Œä¸æˆªæ–­
        request_data = asdict(request_info)
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        cache_size_bytes = sys.getsizeof(self.request_details_cache)
        cache_size_mb = cache_size_bytes / (1024 * 1024)
        
        # å¦‚æœç¼“å­˜è¿‡å¤§ï¼ˆè¶…è¿‡500MBï¼‰ï¼Œåˆ é™¤æœ€è€çš„10%é¡¹ç›®
        if cache_size_mb > self.cache_size_limit_mb and len(self.request_details_cache) > 0:
            # åˆ é™¤æœ€è€çš„10%é¡¹ç›®
            items_to_remove = max(1, len(self.request_details_cache) // 10)
            for _ in range(items_to_remove):
                self.request_details_cache.popitem(last=False)
            cache_size_bytes = sys.getsizeof(self.request_details_cache)
            cache_size_mb = cache_size_bytes / (1024 * 1024)
            logger.info(f"[CACHE] ç¼“å­˜è¶…è¿‡é™åˆ¶ï¼Œå·²æ¸…ç† {items_to_remove} ä¸ªæ—§é¡¹ï¼Œå½“å‰å¤§å°: ~{cache_size_mb:.2f}MB")
        
        # é™åˆ¶ç¼“å­˜é¡¹æ•°
        if len(self.request_details_cache) >= self.MAX_DETAILS_CACHE:
            # åˆ é™¤æœ€è€çš„ç¼“å­˜é¡¹ï¼ˆFIFOï¼‰
            self.request_details_cache.popitem(last=False)
        
        # å­˜å‚¨æ–°é¡¹ - ä¿æŒæ•°æ®å®Œæ•´
        self.request_details_cache[request_id] = request_data
        
        # å®šæœŸè®°å½•ç¼“å­˜çŠ¶æ€ï¼ˆæ¯500ä¸ªè¯·æ±‚ï¼‰
        if len(self.request_details_cache) % 500 == 0:
            logger.debug(f"[CACHE] è¯¦æƒ…ç¼“å­˜çŠ¶æ€ - é¡¹æ•°: {len(self.request_details_cache)}, å¤§å°: ~{cache_size_mb:.2f}MB")
    
    def get_request_details(self, request_id: str) -> Optional[dict]:
        """è·å–è¯·æ±‚è¯¦æƒ…"""
        with self._lock:
            # å…ˆä»ç¼“å­˜ä¸­æŸ¥æ‰¾
            if request_id in self.request_details_cache:
                return self.request_details_cache[request_id]
            
            # ä»æ´»è·ƒè¯·æ±‚ä¸­æŸ¥æ‰¾
            if request_id in self.active_requests:
                return asdict(self.active_requests[request_id])
            
            # ä»æœ€è¿‘è¯·æ±‚ä¸­æŸ¥æ‰¾
            for req in self.recent_requests:
                if req.get('request_id') == request_id:
                    return req
            
            # å¦‚æœå†…å­˜ä¸­éƒ½æ²¡æœ‰ï¼Œä»æ—¥å¿—æ–‡ä»¶ä¸­æŸ¥æ‰¾
            return self._find_request_in_logs(request_id)
    
    def _find_request_in_logs(self, request_id: str) -> Optional[dict]:
        """ä»æ—¥å¿—æ–‡ä»¶ä¸­æŸ¥æ‰¾è¯·æ±‚è¯¦æƒ…ï¼ˆæ”¯æŒåˆ†å±‚æ—¥å¿—å’ŒJSONLæ ¼å¼ï¼‰"""
        try:
            # ä¼˜å…ˆä»åˆ†å±‚æ—¥å¿—ä¸­æŸ¥æ‰¾ï¼ˆæ–°æ ¼å¼ï¼‰
            if MonitorConfig.ENABLE_HIERARCHICAL_LOGS:
                result = self._find_request_in_hierarchical_logs(request_id)
                if result:
                    return result
            
            # ä»SQLiteæ•°æ®åº“æŸ¥æ‰¾
            if self.log_manager.sqlite_logger:
                try:
                    result = self.log_manager.sqlite_logger.get_request_details(request_id)
                    if result:
                        return result
                except Exception as e:
                    logger.warning(f"ä»SQLiteæŸ¥æ‰¾è¯·æ±‚è¯¦æƒ…å¤±è´¥: {e}")
            
            # å›é€€åˆ°æ—§çš„JSONLæ–‡ä»¶æŸ¥æ‰¾
            if self.log_manager.request_log_path.exists():
                with open(self.log_manager.request_log_path, 'r', encoding='utf-8') as f:
                    # ä»åå¾€å‰è¯»å–ï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡
                    lines = f.readlines()
                    for line in reversed(lines):
                        try:
                            log_entry = json.loads(line.strip())
                            if (log_entry.get('request_id') == request_id and
                                log_entry.get('type') == 'request_end'):
                                # æ‰¾åˆ°äº†å®Œæ•´çš„è¯·æ±‚è®°å½•
                                return log_entry
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            logger.error(f"ä»æ—¥å¿—æ–‡ä»¶æŸ¥æ‰¾è¯·æ±‚è¯¦æƒ…å¤±è´¥: {e}")
        
        return None
    
    def _find_request_in_hierarchical_logs(self, request_id: str) -> Optional[dict]:
        """ä»åˆ†å±‚æ—¥å¿—ä¸­æŸ¥æ‰¾è¯·æ±‚è¯¦æƒ…"""
        try:
            req_id_short = request_id[:8] if request_id else "unknown"
            
            # è·å–æœ€è¿‘7å¤©çš„æ—¥æœŸåˆ—è¡¨
            today = datetime.now()
            for i in range(7):
                date = today - timedelta(days=i)
                date_str = date.strftime("%Y%m%d")
                date_dir = MonitorConfig.LOG_DIR / date_str
                
                if not date_dir.exists():
                    continue
                
                # éå†è¯¥æ—¥æœŸä¸‹çš„æ‰€æœ‰å°æ—¶æ–‡ä»¶å¤¹
                for hour_dir in date_dir.iterdir():
                    if not hour_dir.is_dir():
                        continue
                    
                    # æŸ¥æ‰¾åŒ…å«è¯¥request_idçš„æ–‡ä»¶
                    pattern = f"*_{req_id_short}.json"
                    if MonitorConfig.USE_COMPRESSION:
                        pattern = f"*_{req_id_short}.json.gz"
                    
                    for log_file in hour_dir.glob(pattern):
                        try:
                            if MonitorConfig.USE_COMPRESSION:
                                with gzip.open(log_file, 'rt', encoding='utf-8') as f:
                                    log_entry = json.load(f)
                            else:
                                with open(log_file, 'r', encoding='utf-8') as f:
                                    log_entry = json.load(f)
                            
                            # éªŒè¯request_idå®Œå…¨åŒ¹é…
                            if log_entry.get('request_id') == request_id:
                                logger.debug(f"ä»åˆ†å±‚æ—¥å¿—æ‰¾åˆ°è¯·æ±‚è¯¦æƒ…: {log_file}")
                                return log_entry
                                
                        except Exception as e:
                            logger.warning(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
                            continue
            
            return None
            
        except Exception as e:
            logger.error(f"ä»åˆ†å±‚æ—¥å¿—æŸ¥æ‰¾è¯·æ±‚è¯¦æƒ…å¤±è´¥: {e}", exc_info=True)
            return None
    
    def _persist_stats(self):
        """æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶ï¼ˆåŒ…å«æ¯æ—¥ç»Ÿè®¡ï¼‰"""
        try:
            stats_path = MonitorConfig.LOG_DIR / MonitorConfig.STATS_FILE
            
            # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šè®¡ç®—æ¯æ—¥ç»Ÿè®¡
            daily_stats = {}
            for req in self.recent_requests:
                timestamp = req.get('timestamp', 0)
                if timestamp:
                    date_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
                    if date_str not in daily_stats:
                        daily_stats[date_str] = {
                            'total': 0,
                            'success': 0,
                            'failed': 0
                        }
                    
                    daily_stats[date_str]['total'] += 1
                    if req.get('status') == 'success':
                        daily_stats[date_str]['success'] += 1
                    else:
                        daily_stats[date_str]['failed'] += 1
            
            # å‡†å¤‡è¦ä¿å­˜çš„æ•°æ®
            stats_data = {
                'last_update': time.time(),
                'startup_time': self.startup_time,
                'model_stats': dict(self.model_stats),
                # ä¿å­˜æ€»ä½“ç»Ÿè®¡
                'total_requests_all_time': sum(s['total'] for s in self.model_stats.values()),
                'total_success_all_time': sum(s['success'] for s in self.model_stats.values()),
                'total_failed_all_time': sum(s['failed'] for s in self.model_stats.values()),
                # ğŸ”§ æ–°å¢ï¼šä¿å­˜æ¯æ—¥ç»Ÿè®¡
                'daily_stats': daily_stats
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def _load_persisted_stats(self):
        """ä»æ–‡ä»¶åŠ è½½æŒä¹…åŒ–çš„ç»Ÿè®¡æ•°æ®"""
        try:
            stats_path = MonitorConfig.LOG_DIR / MonitorConfig.STATS_FILE
            
            if not stats_path.exists():
                logger.info("æœªæ‰¾åˆ°æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®ï¼Œå°†ä»é›¶å¼€å§‹")
                return
            
            with open(stats_path, 'r', encoding='utf-8') as f:
                stats_data = json.load(f)
            
            # æ¢å¤æ¨¡å‹ç»Ÿè®¡
            if 'model_stats' in stats_data:
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ¨¡å‹ç»Ÿè®¡åŒ…å«å¿…éœ€å­—æ®µ
                loaded_stats = {}
                for model, stats in stats_data['model_stats'].items():
                    loaded_stats[model] = {
                        'total': stats.get('total', 0),
                        'success': stats.get('success', 0),
                        'failed': stats.get('failed', 0),
                        'total_duration': stats.get('total_duration', 0),
                        'count_with_duration': stats.get('count_with_duration', 0)
                    }
                
                self.model_stats = defaultdict(
                    lambda: {'total': 0, 'success': 0, 'failed': 0,
                            'total_duration': 0, 'count_with_duration': 0},
                    loaded_stats
                )
            
            # æ¢å¤æœ€è¿‘çš„è¯·æ±‚å’Œé”™è¯¯
            if 'recent_requests' in stats_data:
                for req in stats_data['recent_requests']:
                    self.recent_requests.append(req)
            
            if 'recent_errors' in stats_data:
                for err in stats_data['recent_errors']:
                    self.recent_errors.append(err)
            
            # å¦‚æœæ˜¯åŒä¸€æ¬¡è¿è¡Œä¼šè¯ï¼Œä¿æŒåŸæœ‰çš„å¯åŠ¨æ—¶é—´
            # å¦åˆ™é‡ç½®å¯åŠ¨æ—¶é—´
            if 'startup_time' in stats_data:
                time_since_last_update = time.time() - stats_data.get('last_update', 0)
                # å¦‚æœè·ç¦»ä¸Šæ¬¡æ›´æ–°è¶…è¿‡1å°æ—¶ï¼Œè®¤ä¸ºæ˜¯æ–°çš„ä¼šè¯
                if time_since_last_update > 3600:
                    self.startup_time = time.time()
                else:
                    self.startup_time = stats_data['startup_time']
            
            logger.info(f"å·²åŠ è½½æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®ï¼š{len(self.model_stats)} ä¸ªæ¨¡å‹ç»Ÿè®¡")
            
        except Exception as e:
            logger.error(f"åŠ è½½æŒä¹…åŒ–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def get_all_time_stats(self) -> dict:
        """è·å–æ‰€æœ‰æ—¶é—´çš„ç»Ÿè®¡æ•°æ®ï¼ˆä»æ—¥å¿—æ–‡ä»¶è®¡ç®—ï¼‰"""
        try:
            if not self.log_manager.request_log_path.exists():
                return {
                    'total_requests': 0,
                    'total_success': 0,
                    'total_failed': 0,
                    'models': {}
                }
            
            model_counts = defaultdict(lambda: {'total': 0, 'success': 0, 'failed': 0})
            total_requests = 0
            total_success = 0
            total_failed = 0
            
            with open(self.log_manager.request_log_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        log_entry = json.loads(line.strip())
                        if log_entry.get('type') == 'request_end':
                            model = log_entry.get('model', 'unknown')
                            status = log_entry.get('status', 'failed')
                            
                            total_requests += 1
                            model_counts[model]['total'] += 1
                            
                            if status == 'success':
                                total_success += 1
                                model_counts[model]['success'] += 1
                            else:
                                total_failed += 1
                                model_counts[model]['failed'] += 1
                                
                    except json.JSONDecodeError:
                        continue
            
            return {
                'total_requests': total_requests,
                'total_success': total_success,
                'total_failed': total_failed,
                'models': dict(model_counts)
            }
            
        except Exception as e:
            logger.error(f"è®¡ç®—æ‰€æœ‰æ—¶é—´ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'total_requests': 0,
                'total_success': 0,
                'total_failed': 0,
                'models': {}
            }

# åˆ›å»ºå…¨å±€ç›‘æ§æœåŠ¡å®ä¾‹
monitoring_service = MonitoringService()