"""
å›¾ç‰‡å¤„ç†æœåŠ¡æ¨¡å—
å¤„ç†å›¾ç‰‡ä¸‹è½½ã€ä¼˜åŒ–ã€ä¸Šä¼ å›¾åºŠã€ç¼“å­˜ç­‰åŠŸèƒ½
"""

import asyncio
import hashlib
import logging
import time
import uuid
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Optional, Tuple

import aiohttp
import requests
from PIL import Image

from modules.file_uploader import upload_to_file_bed
from modules.image_processor import (
    optimize_image,
    image_to_base64,
    get_mime_type_from_format,
    decode_base64_image,
    merge_image_config
)

logger = logging.getLogger(__name__)

# --- å›¾ç‰‡è‡ªåŠ¨ä¸‹è½½é…ç½® ---
IMAGE_SAVE_DIR = Path("./downloaded_images")
IMAGE_SAVE_DIR.mkdir(exist_ok=True)


def calculate_image_hash(base64_data: str) -> str:
    """è®¡ç®—å›¾ç‰‡å†…å®¹çš„SHA256 hashï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
    # ç§»é™¤data URIå‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if ',' in base64_data:
        _, data_only = base64_data.split(',', 1)
    else:
        data_only = base64_data
    # è®¡ç®—hashï¼ˆä½¿ç”¨base64å­—ç¬¦ä¸²ï¼Œé¿å…è§£ç å¼€é”€ï¼‰
    return hashlib.sha256(data_only.encode('utf-8')).hexdigest()


async def save_image_data(image_data, url, request_id, CONFIG):
    """ä¿å­˜å›¾ç‰‡æ•°æ®åˆ°æ–‡ä»¶ï¼ˆå¼‚æ­¥ï¼‰"""
    try:
        original_size_kb = len(image_data) / 1024
        
        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹
        date_folder = datetime.now().strftime("%Y%m%d")
        date_path = IMAGE_SAVE_DIR / date_folder
        date_path.mkdir(exist_ok=True)
        logger.info(f"ğŸ“ ä½¿ç”¨æ—¥æœŸæ–‡ä»¶å¤¹: {date_folder}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ ¼å¼è½¬æ¢ï¼ˆæœ¬åœ°ä¿å­˜ï¼‰
        local_format_config = CONFIG.get("local_save_format", {})
        target_ext = 'png'  # é»˜è®¤æ‰©å±•å
        
        if local_format_config.get("enabled", False):
            target_format = local_format_config.get("format", "original").lower()
            
            if target_format != "original":
                try:
                    # æ‰“å¼€å›¾ç‰‡
                    img = Image.open(BytesIO(image_data))
                    
                    # å¦‚æœæ˜¯RGBAæ¨¡å¼ä¸”è¦è½¬æ¢ä¸ºJPEGï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºRGB
                    if target_format in ['jpeg', 'jpg'] and img.mode in ('RGBA', 'LA', 'P'):
                        # åˆ›å»ºç™½è‰²èƒŒæ™¯
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        if img.mode == 'P':
                            img = img.convert('RGBA')
                        background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                        img = background
                    
                    # ä¿å­˜åˆ°BytesIO
                    output = BytesIO()
                    
                    # æ ¹æ®ç›®æ ‡æ ¼å¼ä¿å­˜
                    if target_format == 'png':
                        img.save(output, format='PNG', optimize=True)
                        target_ext = 'png'
                    elif target_format in ['jpeg', 'jpg']:
                        # æœ¬åœ°ä¿å­˜ä½¿ç”¨é«˜è´¨é‡
                        jpeg_quality = local_format_config.get("jpeg_quality", 100)
                        img.save(output, format='JPEG', quality=jpeg_quality, optimize=True)
                        target_ext = 'jpg'
                    elif target_format == 'webp':
                        img.save(output, format='WEBP', quality=95, optimize=True)
                        target_ext = 'webp'
                    else:
                        # ä¸æ”¯æŒçš„æ ¼å¼ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                        output = BytesIO(image_data)
                        # ä»URLæ¨æ–­æ‰©å±•å
                        if '.jpeg' in url.lower():
                            target_ext = 'jpeg'
                        elif '.jpg' in url.lower():
                            target_ext = 'jpg'
                        elif '.png' in url.lower():
                            target_ext = 'png'
                        elif '.webp' in url.lower():
                            target_ext = 'webp'
                    
                    # è·å–è½¬æ¢åçš„æ•°æ®
                    image_data = output.getvalue()
                    
                    converted_size_kb = len(image_data) / 1024
                    logger.info(f"ğŸ”„ æœ¬åœ°ä¿å­˜å·²è½¬æ¢ä¸º {target_format.upper()} æ ¼å¼ï¼ˆ{original_size_kb:.1f}KB â†’ {converted_size_kb:.1f}KBï¼‰")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æœ¬åœ°ä¿å­˜æ ¼å¼è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ ¼å¼")
                    # ä»URLæ¨æ–­æ‰©å±•å
                    if '.jpeg' in url.lower():
                        target_ext = 'jpeg'
                    elif '.jpg' in url.lower():
                        target_ext = 'jpg'
                    elif '.png' in url.lower():
                        target_ext = 'png'
                    elif '.webp' in url.lower():
                        target_ext = 'webp'
            else:
                # ä¿æŒåŸæ ¼å¼ï¼Œä»URLæ¨æ–­æ‰©å±•å
                if '.jpeg' in url.lower():
                    target_ext = 'jpeg'
                elif '.jpg' in url.lower():
                    target_ext = 'jpg'
                elif '.png' in url.lower():
                    target_ext = 'png'
                elif '.webp' in url.lower():
                    target_ext = 'webp'
        else:
            # æœªå¯ç”¨æ ¼å¼è½¬æ¢ï¼Œä»URLæ¨æ–­æ‰©å±•å
            if '.jpeg' in url.lower():
                target_ext = 'jpeg'
            elif '.jpg' in url.lower():
                target_ext = 'jpg'
            elif '.png' in url.lower():
                target_ext = 'png'
            elif '.webp' in url.lower():
                target_ext = 'webp'
            elif '.' in url:
                possible_ext = url.split('.')[-1].split('?')[0].lower()
                if possible_ext in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                    target_ext = possible_ext
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # æ·»åŠ æ¯«ç§’
        
        # ä½¿ç”¨æ—¶é—´æˆ³å’Œè¯·æ±‚IDä½œä¸ºæ–‡ä»¶å
        filename = f"{timestamp}_{request_id[:8]}.{target_ext}"
        filepath = date_path / filename  # ä½¿ç”¨æ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
        
        # å¼‚æ­¥ä¿å­˜æ–‡ä»¶
        await asyncio.get_event_loop().run_in_executor(None, filepath.write_bytes, image_data)
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        size_kb = len(image_data) / 1024
        size_mb = size_kb / 1024
        
        if size_mb > 1:
            logger.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filename} ({size_mb:.2f}MB)")
        else:
            logger.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filename} ({size_kb:.1f}KB)")
        
        # æ˜¾ç¤ºå®Œæ•´è·¯å¾„
        logger.info(f"   ğŸ“ ä¿å­˜ä½ç½®: {filepath.absolute()}")
            
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")


async def save_downloaded_image_async(image_data, url, request_id, downloaded_urls_set, CONFIG):
    """ä¿å­˜å·²ä¸‹è½½çš„å›¾ç‰‡æ•°æ®åˆ°æœ¬åœ°ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰"""
    # é¿å…é‡å¤ä¿å­˜
    if url in downloaded_urls_set:
        show_full_urls = CONFIG.get("debug_show_full_urls", False)
        url_display = url if show_full_urls else url[:CONFIG.get("url_display_length", 200)]
        logger.info(f"ğŸ¨ å›¾ç‰‡å·²å­˜åœ¨è®°å½•ï¼Œè·³è¿‡ä¿å­˜: {url_display}{'...' if not show_full_urls and len(url) > CONFIG.get('url_display_length', 200) else ''}")
        return
    
    try:
        # ç›´æ¥ä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®ä¿å­˜ï¼Œé¿å…é‡å¤ä¸‹è½½
        await save_image_data(image_data, url, request_id, CONFIG)
        
        # æ›´æ–°å·²ä¸‹è½½è®°å½•ï¼ˆç”±è°ƒç”¨æ–¹å¤„ç†ï¼‰
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {type(e).__name__}: {e}")


async def download_image_data_with_retry(
    url: str, 
    aiohttp_session: aiohttp.ClientSession,
    DOWNLOAD_SEMAPHORE: asyncio.Semaphore,
    MAX_CONCURRENT_DOWNLOADS: int,
    CONFIG: dict
) -> Tuple[Optional[bytes], Optional[str]]:
    """ä¼˜åŒ–çš„å¼‚æ­¥å›¾ç‰‡ä¸‹è½½å™¨ï¼Œå¸¦é‡è¯•å’Œå¹¶å‘æ§åˆ¶"""
    if not DOWNLOAD_SEMAPHORE:
        DOWNLOAD_SEMAPHORE = asyncio.Semaphore(MAX_CONCURRENT_DOWNLOADS)
    
    last_error = None
    max_retries = CONFIG.get("download_timeout", {}).get("max_retries", 2)
    retry_delays = [1, 2]  # å‡å°‘é‡è¯•å»¶è¿Ÿ
    
    # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šå¹¶å‘æ§åˆ¶çŠ¶æ€
    semaphore_available = DOWNLOAD_SEMAPHORE._value if DOWNLOAD_SEMAPHORE else 0
    logger.info(f"[DOWNLOAD_DEBUG] å‡†å¤‡ä¸‹è½½å›¾ç‰‡")
    logger.info(f"  - å¯ç”¨ä¸‹è½½æ§½: {semaphore_available}/{MAX_CONCURRENT_DOWNLOADS}")
    logger.info(f"  - æ´»è·ƒä¸‹è½½: {MAX_CONCURRENT_DOWNLOADS - semaphore_available}")
    logger.info(f"  - æœ€å¤§é‡è¯•: {max_retries}")
    logger.info(f"  - URLå‰100å­—ç¬¦: {url[:100]}...")
    
    # ğŸ”§ ä¸‹è½½å»¶è¿Ÿæœºåˆ¶ï¼ˆé¿å…TCPç«¯å£è€—å°½ï¼‰
    delay_config = CONFIG.get("download_delay", {})
    if delay_config.get("enabled", False):
        delay_seconds = delay_config.get("delay_seconds", 0.5)
        logger.info(f"[DOWNLOAD_DEBUG] â±ï¸ å»¶è¿Ÿ {delay_seconds} ç§’åå¼€å§‹ï¼ˆé¿å…å¹¶å‘å†²çªï¼‰")
        await asyncio.sleep(delay_seconds)
    
    # è®°å½•ç­‰å¾…ä¿¡å·é‡çš„æ—¶é—´
    import time as time_module
    wait_start = time_module.time()
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    async with DOWNLOAD_SEMAPHORE:
        wait_time = time_module.time() - wait_start
        if wait_time > 1:
            logger.warning(f"[DOWNLOAD_DEBUG] âš ï¸ ç­‰å¾…ä¸‹è½½æ§½è€—æ—¶: {wait_time:.2f}ç§’ï¼ˆå¹¶å‘é˜»å¡ï¼ï¼‰")
        for retry_count in range(max_retries):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Referer': 'https://lmarena.ai/'
                }
                
                if not aiohttp_session:
                    # ğŸ”§ åˆ›å»ºç´§æ€¥ä¼šè¯ï¼ˆä½¿ç”¨ç›¸åŒçš„SSLé…ç½®ï¼‰
                    import ssl
                    ssl_context = ssl.create_default_context()
                    ssl_context.check_hostname = False
                    ssl_context.verify_mode = ssl.CERT_NONE
                    connector = aiohttp.TCPConnector(ssl=ssl_context, limit=100, limit_per_host=30)
                    aiohttp_session = aiohttp.ClientSession(connector=connector)
                    logger.warning("[DOWNLOAD_DEBUG] åˆ›å»ºäº†ç´§æ€¥aiohttpä¼šè¯ï¼ˆä½¿ç”¨è‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡ï¼‰")
                
                # ä¼˜åŒ–çš„è¶…æ—¶è®¾ç½®ï¼ˆä»é…ç½®è¯»å–ï¼‰
                timeout_config = CONFIG.get("download_timeout", {})
                timeout = aiohttp.ClientTimeout(
                    total=timeout_config.get("total", 30),
                    connect=timeout_config.get("connect", 5),
                    sock_read=timeout_config.get("sock_read", 10)
                )
                
                # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè¶…æ—¶é…ç½®
                logger.info(f"[DOWNLOAD_DEBUG] é‡è¯• #{retry_count + 1}/{max_retries}")
                logger.info(f"  - è¿æ¥è¶…æ—¶: {timeout_config.get('connect', 5)}ç§’")
                logger.info(f"  - è¯»å–è¶…æ—¶: {timeout_config.get('sock_read', 10)}ç§’")
                logger.info(f"  - æ€»è¶…æ—¶: {timeout_config.get('total', 30)}ç§’")
                
                # æ·»åŠ æ€§èƒ½æ—¥å¿—
                import time as time_module
                start_time = time_module.time()
                
                # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè¿æ¥å¼€å§‹
                logger.info(f"[DOWNLOAD_DEBUG] å¼€å§‹å»ºç«‹è¿æ¥...")
                
                async with aiohttp_session.get(
                    url,
                    timeout=timeout,
                    headers=headers,
                    allow_redirects=True
                ) as response:
                    connect_time = time_module.time() - start_time
                    logger.info(f"[DOWNLOAD_DEBUG] è¿æ¥å»ºç«‹æˆåŠŸï¼Œè€—æ—¶: {connect_time:.2f}ç§’")
                    
                    if response.status == 200:
                        logger.info(f"[DOWNLOAD_DEBUG] HTTP 200 OKï¼Œå¼€å§‹è¯»å–æ•°æ®...")
                        read_start = time_module.time()
                        data = await response.read()
                        read_time = time_module.time() - read_start
                        download_time = time_module.time() - start_time
                        
                        # ğŸ” è¯¦ç»†æ€§èƒ½åˆ†æ
                        logger.info(f"[DOWNLOAD_DEBUG] ä¸‹è½½å®Œæˆ")
                        logger.info(f"  - è¿æ¥æ—¶é—´: {connect_time:.2f}ç§’")
                        logger.info(f"  - è¯»å–æ—¶é—´: {read_time:.2f}ç§’")
                        logger.info(f"  - æ€»æ—¶é—´: {download_time:.2f}ç§’")
                        logger.info(f"  - æ•°æ®å¤§å°: {len(data) / 1024:.1f}KB")
                        logger.info(f"  - ä¸‹è½½é€Ÿåº¦: {(len(data) / 1024) / download_time:.1f}KB/s")
                        
                        # è®°å½•æ…¢é€Ÿä¸‹è½½
                        slow_threshold = CONFIG.get("performance_monitoring", {}).get("slow_threshold_seconds", 10)
                        if download_time > slow_threshold:
                            logger.warning(f"[DOWNLOAD] âš ï¸ ä¸‹è½½è€—æ—¶è¾ƒé•¿: {download_time:.2f}ç§’ (é˜ˆå€¼: {slow_threshold}ç§’)")
                        
                        return data, None
                    else:
                        last_error = f"HTTP {response.status}"
                        logger.error(f"[DOWNLOAD_DEBUG] âŒ HTTPé”™è¯¯: {response.status}")
                        
            except asyncio.TimeoutError as e:
                elapsed = time_module.time() - start_time
                last_error = f"è¶…æ—¶ï¼ˆç¬¬{retry_count+1}æ¬¡å°è¯•ï¼‰"
                logger.error(f"[DOWNLOAD_DEBUG] âŒ è¶…æ—¶")
                logger.error(f"  - å·²ç­‰å¾…: {elapsed:.2f}ç§’")
                logger.error(f"  - é…ç½®æ€»è¶…æ—¶: {timeout_config.get('total', 30)}ç§’")
                logger.error(f"  - å¯èƒ½åŸå› : ç½‘ç»œæ…¢ã€æœåŠ¡å™¨å“åº”æ…¢ã€æˆ–æ•°æ®é‡å¤§")
            except aiohttp.ClientError as e:
                elapsed = time_module.time() - start_time
                last_error = f"ç½‘ç»œé”™è¯¯: {str(e)}"
                logger.error(f"[DOWNLOAD_DEBUG] âŒ ç½‘ç»œé”™è¯¯: {e.__class__.__name__}")
                logger.error(f"  - é”™è¯¯è¯¦æƒ…: {str(e)[:200]}")
                logger.error(f"  - å‘ç”Ÿæ—¶é—´: {elapsed:.2f}ç§’å")
                # ğŸ” è¯Šæ–­SSLé”™è¯¯
                if "SSL" in str(e) or "ssl" in str(e).lower():
                    logger.error(f"  - ğŸ’¡ æ£€æµ‹åˆ°SSLé”™è¯¯ï¼Œå¯èƒ½æ˜¯è¯ä¹¦é—®é¢˜æˆ–é˜²ç«å¢™æ‹¦æˆª")
            except Exception as e:
                elapsed = time_module.time() - start_time
                last_error = str(e)
                logger.error(f"[DOWNLOAD_DEBUG] âŒ æœªçŸ¥é”™è¯¯: {e}")
                logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"  - å‘ç”Ÿæ—¶é—´: {elapsed:.2f}ç§’å")
            
            # é‡è¯•å»¶è¿Ÿ
            if retry_count < max_retries - 1:
                delay = retry_delays[retry_count]
                logger.info(f"[DOWNLOAD_DEBUG] ç­‰å¾…{delay}ç§’åé‡è¯•...")
                await asyncio.sleep(delay)
    
    return None, last_error


async def process_image_data(
    base64_data: str,
    filename: str = None,
    request_id: str = None,
    CONFIG: dict = None,
    PROCESSED_IMAGE_CACHE: dict = None,
    DISABLED_ENDPOINTS: dict = None,
    ROUND_ROBIN_INDEX_REF: list = None,  # ä½¿ç”¨åˆ—è¡¨ä½œä¸ºå¯å˜å¼•ç”¨
    FILEBED_RECOVERY_TIME: int = 300,
    model_image_config: dict = None  # æ–°å¢ï¼šæ¨¡å‹çº§åˆ«çš„å›¾ç‰‡å‹ç¼©é…ç½®
) -> Tuple[str, Optional[str]]:
    """
    ç»Ÿä¸€çš„å›¾ç‰‡å¤„ç†å‡½æ•°ï¼Œæ ¹æ®é…ç½®å†³å®šå¤„ç†æµç¨‹
    
    æ”¯æŒå››ç§é…ç½®ç»„åˆï¼š
    1. file_bed_enabled=True + image_optimization.enabled=True: ä¼˜åŒ– -> å›¾åºŠURL
    2. file_bed_enabled=True + image_optimization.enabled=False: åŸå›¾ -> å›¾åºŠURL
    3. file_bed_enabled=False + image_optimization.enabled=True: ä¼˜åŒ– -> base64
    4. file_bed_enabled=False + image_optimization.enabled=False: åŸå›¾ -> base64
    
    Args:
        base64_data: åŸå§‹base64å›¾ç‰‡æ•°æ®ï¼ˆå¯ä»¥æ˜¯Data URIæ ¼å¼ï¼‰
        filename: å¯é€‰çš„æ–‡ä»¶å
        request_id: è¯·æ±‚IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
        CONFIG: é…ç½®å­—å…¸
        PROCESSED_IMAGE_CACHE: å¤„ç†å›¾ç‰‡ç¼“å­˜å­—å…¸
        DISABLED_ENDPOINTS: ç¦ç”¨ç«¯ç‚¹å­—å…¸
        ROUND_ROBIN_INDEX_REF: è½®è¯¢ç´¢å¼•å¼•ç”¨ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰
        FILEBED_RECOVERY_TIME: å›¾åºŠæ¢å¤æ—¶é—´
        model_image_config: æ¨¡å‹çº§åˆ«çš„å›¾ç‰‡å‹ç¼©é…ç½®ï¼Œä¼˜å…ˆçº§é«˜äºå…¨å±€é…ç½®
            ç¤ºä¾‹: {
                "enabled": true,
                "convert_png_to_jpg": true,
                "target_format": "jpg",
                "quality": 80,
                "target_size_kb": 500,
                "max_width": 1920,
                "max_height": 1080
            }
        
    Returns:
        (å¤„ç†åçš„æ•°æ®, é”™è¯¯ä¿¡æ¯)
        - å¦‚æœæˆåŠŸï¼Œè¿”å› (URLæˆ–base64å­—ç¬¦ä¸², None)
        - å¦‚æœå¤±è´¥ï¼Œè¿”å› (åŸå§‹æ•°æ®, é”™è¯¯æ¶ˆæ¯)
    """
    if not filename:
        filename = f"image_{uuid.uuid4()}.png"
    
    req_log = f"[IMG_PROC {request_id[:8] if request_id else 'N/A'}]"
    
    # è¯»å–å…¨å±€é…ç½®
    file_bed_enabled = CONFIG.get("file_bed_enabled", False)
    global_optimization_config = CONFIG.get("image_optimization", {})
    cache_config = CONFIG.get("processed_image_cache", {})
    cache_enabled = cache_config.get("enabled", True)
    
    # åˆå¹¶æ¨¡å‹çº§åˆ«é…ç½®ï¼ˆæ¨¡å‹é…ç½®ä¼˜å…ˆçº§æ›´é«˜ï¼‰
    optimization_config = merge_image_config(global_optimization_config, model_image_config)
    optimization_enabled = optimization_config.get("enabled", False)
    
    # å¦‚æœæ¨¡å‹é…ç½®ä¸­æ˜¾å¼å¯ç”¨äº†å‹ç¼©ï¼Œè¦†ç›–å…¨å±€è®¾ç½®
    if model_image_config and model_image_config.get("enabled", False):
        optimization_enabled = True
    
    logger.info(f"{req_log} å¼€å§‹å¤„ç†å›¾ç‰‡: file_bed={file_bed_enabled}, optimization={optimization_enabled}, cache={cache_enabled}")
    if model_image_config:
        logger.info(f"{req_log} ä½¿ç”¨æ¨¡å‹çº§åˆ«å›¾ç‰‡é…ç½®: {model_image_config}")
    
    # --- ç¼“å­˜é€»è¾‘ ---
    image_hash = None
    if cache_enabled and PROCESSED_IMAGE_CACHE is not None:
        image_hash = calculate_image_hash(base64_data)
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        if image_hash in PROCESSED_IMAGE_CACHE:
            cached_data, cache_time = PROCESSED_IMAGE_CACHE[image_hash]
            ttl = cache_config.get("ttl_seconds", 3600)
            if current_time - cache_time < ttl:
                logger.info(f"{req_log} âš¡ å‘½ä¸­ç¼“å­˜ (hash: {image_hash[:8]}...)")
                return cached_data, None
            else:
                logger.info(f"{req_log} ç¼“å­˜å·²è¿‡æœŸ (hash: {image_hash[:8]}...)")
                del PROCESSED_IMAGE_CACHE[image_hash]
    
    try:
        # è§£ç base64æ•°æ®
        image_bytes, image_format, decode_error = decode_base64_image(base64_data)
        if decode_error:
            logger.error(f"{req_log} è§£ç å¤±è´¥: {decode_error}")
            return base64_data, decode_error
        
        # ç¡®å®šå¤„ç†åçš„å›¾ç‰‡æ•°æ®å’Œæ ¼å¼
        final_image_data = image_bytes
        final_format = image_format
        
        # æ­¥éª¤1: å›¾ç‰‡ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if optimization_enabled:
            logger.info(f"{req_log} æ‰§è¡Œå›¾ç‰‡ä¼˜åŒ–...")
            optimized_data, optimized_format, opt_error = optimize_image(
                image_bytes, 
                optimization_config,
                image_format
            )
            
            if opt_error:
                logger.warning(f"{req_log} ä¼˜åŒ–å¤±è´¥: {opt_error}ï¼Œä½¿ç”¨åŸå›¾")
                # ä¼˜åŒ–å¤±è´¥ï¼Œé™çº§ä½¿ç”¨åŸå›¾
            else:
                final_image_data = optimized_data
                final_format = optimized_format
                logger.info(f"{req_log} ä¼˜åŒ–æˆåŠŸ: {len(image_bytes)/1024:.1f}KB -> {len(final_image_data)/1024:.1f}KB")
        else:
            logger.info(f"{req_log} è·³è¿‡å›¾ç‰‡ä¼˜åŒ–ï¼ˆé…ç½®å·²ç¦ç”¨ï¼‰")
        
        # æ­¥éª¤2: æ ¹æ®file_bed_enabledå†³å®šè¾“å‡ºæ–¹å¼
        if file_bed_enabled:
            logger.info(f"{req_log} ä¸Šä¼ åˆ°å›¾åºŠ...")
            
            # å°†å›¾ç‰‡æ•°æ®è½¬æ¢ä¸ºbase64 Data URIç”¨äºä¸Šä¼ 
            mime_type = get_mime_type_from_format(final_format)
            upload_base64 = image_to_base64(final_image_data, mime_type)
            
            # è·å–æ´»è·ƒçš„å›¾åºŠç«¯ç‚¹
            all_endpoints = CONFIG.get("file_bed_endpoints", [])
            current_time = time.time()
            
            # è‡ªåŠ¨æ¢å¤è¶…æ—¶çš„ç«¯ç‚¹
            if DISABLED_ENDPOINTS is not None:
                endpoints_to_recover = []
                for endpoint_name, disable_time in list(DISABLED_ENDPOINTS.items()):
                    if current_time - disable_time > FILEBED_RECOVERY_TIME:
                        endpoints_to_recover.append(endpoint_name)
                
                for endpoint_name in endpoints_to_recover:
                    del DISABLED_ENDPOINTS[endpoint_name]
                    logger.info(f"{req_log} å›¾åºŠç«¯ç‚¹ '{endpoint_name}' å·²è‡ªåŠ¨æ¢å¤")
                
                active_endpoints = [ep for ep in all_endpoints if ep.get("enabled") and ep.get("name") not in DISABLED_ENDPOINTS]
            else:
                active_endpoints = [ep for ep in all_endpoints if ep.get("enabled")]
            
            if not active_endpoints:
                error_msg = "æ²¡æœ‰å¯ç”¨çš„å›¾åºŠç«¯ç‚¹"
                logger.error(f"{req_log} {error_msg}ï¼Œé™çº§è¿”å›base64")
                # é™çº§ï¼šè¿”å›base64
                mime_type = get_mime_type_from_format(final_format)
                return image_to_base64(final_image_data, mime_type), None
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©ç«¯ç‚¹
            import random
            strategy = CONFIG.get("file_bed_selection_strategy", "random")
            
            if ROUND_ROBIN_INDEX_REF and len(ROUND_ROBIN_INDEX_REF) > 0:
                ROUND_ROBIN_INDEX = ROUND_ROBIN_INDEX_REF[0]
                
                if strategy == "failover":
                    start_index = ROUND_ROBIN_INDEX % len(active_endpoints)
                    endpoints_to_try = active_endpoints[start_index:] + active_endpoints[:start_index]
                elif strategy == "round_robin":
                    start_index = ROUND_ROBIN_INDEX % len(active_endpoints)
                    endpoints_to_try = active_endpoints[start_index:] + active_endpoints[:start_index]
                    ROUND_ROBIN_INDEX_REF[0] = ROUND_ROBIN_INDEX + 1
                else:  # random
                    endpoints_to_try = random.sample(active_endpoints, len(active_endpoints))
            else:
                # æ²¡æœ‰è½®è¯¢ç´¢å¼•å¼•ç”¨ï¼Œä½¿ç”¨éšæœºç­–ç•¥
                endpoints_to_try = random.sample(active_endpoints, len(active_endpoints))
            
            # å°è¯•ä¸Šä¼ 
            upload_successful = False
            last_error = None
            final_url = None
            
            for i, endpoint in enumerate(endpoints_to_try):
                endpoint_name = endpoint.get("name", "Unknown")
                
                if DISABLED_ENDPOINTS is not None and endpoint_name in DISABLED_ENDPOINTS:
                    continue
                
                logger.info(f"{req_log} å°è¯•ä¸Šä¼ åˆ° '{endpoint_name}'...")
                
                uploaded_url, upload_error = await upload_to_file_bed(
                    file_name=filename,
                    file_data=upload_base64,
                    endpoint=endpoint
                )
                
                if not upload_error:
                    final_url = uploaded_url
                    upload_successful = True
                    logger.info(f"{req_log} ä¸Šä¼ æˆåŠŸåˆ° '{endpoint_name}': {uploaded_url[:100]}...")
                    break
                else:
                    logger.warning(f"{req_log} ä¸Šä¼ å¤±è´¥åˆ° '{endpoint_name}': {upload_error}")
                    if DISABLED_ENDPOINTS is not None:
                        DISABLED_ENDPOINTS[endpoint_name] = time.time()
                    last_error = upload_error
                    
                    if strategy == "failover" and i == 0 and ROUND_ROBIN_INDEX_REF:
                        ROUND_ROBIN_INDEX_REF[0] += 1
                        logger.info(f"{req_log} [Failover] é»˜è®¤å›¾åºŠå¤±è´¥ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª")
            
            if upload_successful:
                # å­˜å…¥ç¼“å­˜
                if cache_enabled and image_hash and PROCESSED_IMAGE_CACHE is not None:
                    PROCESSED_IMAGE_CACHE[image_hash] = (final_url, time.time())
                    logger.info(f"{req_log} ğŸ’¾ ç»“æœå·²å­˜å…¥ç¼“å­˜ (hash: {image_hash[:8]}...)")
                    # æ£€æŸ¥ç¼“å­˜å¤§å°
                    max_size = cache_config.get("max_size", 200)
                    if len(PROCESSED_IMAGE_CACHE) > max_size:
                        oldest_hash = next(iter(PROCESSED_IMAGE_CACHE))
                        del PROCESSED_IMAGE_CACHE[oldest_hash]
                        logger.info(f"{req_log} ğŸ§¹ ç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§æ¡ç›®")
                return final_url, None
            else:
                error_msg = f"æ‰€æœ‰å›¾åºŠç«¯ç‚¹å‡ä¸Šä¼ å¤±è´¥ã€‚æœ€åé”™è¯¯: {last_error}"
                logger.error(f"{req_log} {error_msg}ï¼Œé™çº§è¿”å›base64")
                # é™çº§ï¼šè¿”å›base64
                mime_type = get_mime_type_from_format(final_format)
                base64_result = image_to_base64(final_image_data, mime_type)
                
                # å³ä½¿å¤±è´¥ä¹Ÿè¦ç¼“å­˜é™çº§ç»“æœ
                if cache_enabled and image_hash and PROCESSED_IMAGE_CACHE is not None:
                    PROCESSED_IMAGE_CACHE[image_hash] = (base64_result, time.time())
                    logger.info(f"{req_log} ğŸ’¾ é™çº§ç»“æœå·²å­˜å…¥ç¼“å­˜ (hash: {image_hash[:8]}...)")

                return base64_result, None
        
        else:
            # ä¸ä½¿ç”¨å›¾åºŠï¼Œè¿”å›base64
            logger.info(f"{req_log} è½¬æ¢ä¸ºbase64ï¼ˆå›¾åºŠå·²ç¦ç”¨ï¼‰")
            mime_type = get_mime_type_from_format(final_format)
            base64_result = image_to_base64(final_image_data, mime_type)
            logger.info(f"{req_log} Base64è½¬æ¢å®Œæˆ: {len(base64_result)} å­—ç¬¦")
            
            # å­˜å…¥ç¼“å­˜
            if cache_enabled and image_hash and PROCESSED_IMAGE_CACHE is not None:
                PROCESSED_IMAGE_CACHE[image_hash] = (base64_result, time.time())
                logger.info(f"{req_log} ğŸ’¾ ç»“æœå·²å­˜å…¥ç¼“å­˜ (hash: {image_hash[:8]}...)")
                # æ£€æŸ¥ç¼“å­˜å¤§å°
                max_size = cache_config.get("max_size", 200)
                if len(PROCESSED_IMAGE_CACHE) > max_size:
                    # ç®€å•åœ°ç§»é™¤æœ€æ—§çš„æ¡ç›® (LRU-like)
                    oldest_hash = next(iter(PROCESSED_IMAGE_CACHE))
                    del PROCESSED_IMAGE_CACHE[oldest_hash]
                    logger.info(f"{req_log} ğŸ§¹ ç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§æ¡ç›®")

            return base64_result, None
    
    except Exception as e:
        error_msg = f"å›¾ç‰‡å¤„ç†å¼‚å¸¸: {type(e).__name__}: {e}"
        logger.error(f"{req_log} {error_msg}", exc_info=True)
        # é™çº§ï¼šè¿”å›åŸå§‹æ•°æ®
        return base64_data, error_msg