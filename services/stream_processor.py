"""
æµå¤„ç†æœåŠ¡æ¨¡å—
å¤„ç†æ¥è‡ªæµè§ˆå™¨çš„åŸå§‹æ•°æ®æµï¼Œå¹¶æ ¼å¼åŒ–ä¸ºOpenAIå…¼å®¹çš„å“åº”
"""

import asyncio
import json
import logging
import re
import time
import uuid
from typing import Optional, Tuple
import mimetypes
import base64

logger = logging.getLogger(__name__)


async def _process_lmarena_stream(request_id: str, queue, request_metadata: dict, CONFIG: dict, 
                                   browser_connections: dict, response_channels: dict,
                                   IS_REFRESHING_FOR_VERIFICATION: bool, VERIFICATION_COOLDOWN_UNTIL: Optional[float],
                                   aiohttp_session, IMAGE_BASE64_CACHE: dict, IMAGE_CACHE_MAX_SIZE: int,
                                   IMAGE_CACHE_TTL: int, save_downloaded_image_async, _download_image_data_with_retry,
                                   release_tab_request):
    """
    æ ¸å¿ƒå†…éƒ¨ç”Ÿæˆå™¨ï¼šå¤„ç†æ¥è‡ªæµè§ˆå™¨çš„åŸå§‹æ•°æ®æµï¼Œå¹¶äº§ç”Ÿç»“æ„åŒ–äº‹ä»¶ã€‚
    äº‹ä»¶ç±»å‹: ('content', str), ('finish', str), ('error', str), ('retry_info', dict)
    """
    # ğŸ”§ ç»ˆæä¿®å¤ï¼šåœ¨å‡½æ•°é¡¶éƒ¨ç¨³å¥åœ°å®šä¹‰å˜é‡
    stream_cancelled = False
    logger.info(f"[STREAM_LIFECYCLE] ğŸš€ _process_lmarena_stream å¼€å§‹å¤„ç†: {request_id[:8]}")
    
    if not queue:
        logger.error(f"PROCESSOR [ID: {request_id[:8]}]: æ— æ³•æ‰¾åˆ°å“åº”é€šé“ã€‚")
        yield 'error', 'Internal server error: response channel not found.'
        return

    buffer = ""
    timeout = CONFIG.get("stream_response_timeout_seconds",360)
    text_pattern = re.compile(r'[ab]0:"((?:\\.|[^"\\])*)"')
    # æ–°å¢ï¼šç”¨äºåŒ¹é…æ€ç»´é“¾å†…å®¹çš„æ­£åˆ™è¡¨è¾¾å¼
    reasoning_pattern = re.compile(r'ag:"((?:\\.|[^"\\])*)"')
    # æ–°å¢ï¼šç”¨äºåŒ¹é…å’Œæå–å›¾ç‰‡URLçš„æ­£åˆ™è¡¨è¾¾å¼
    image_pattern = re.compile(r'[ab]2:(\[.*?\])')
    finish_pattern = re.compile(r'[ab]d:(\{.*?"finishReason".*?\})')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    cloudflare_patterns = [r'<title>Just a moment...</title>', r'Enable JavaScript and cookies to continue']
    
    has_yielded_content = False # æ ‡è®°æ˜¯å¦å·²äº§å‡ºè¿‡æœ‰æ•ˆå†…å®¹
    
    # æ€ç»´é“¾ç›¸å…³å˜é‡
    # æ³¨æ„ï¼šæ€ç»´é“¾æ•°æ®åº”è¯¥æ€»æ˜¯è¢«æ”¶é›†ï¼ˆç”¨äºç›‘æ§å’Œæ—¥å¿—ï¼‰ï¼Œä½†æ˜¯å¦è¾“å‡ºç»™å®¢æˆ·ç«¯ç”±é…ç½®å†³å®š
    enable_reasoning_output = CONFIG.get("enable_lmarena_reasoning", False)  # æ˜¯å¦è¾“å‡ºç»™å®¢æˆ·ç«¯
    reasoning_buffer = []  # ç¼“å†²æ‰€æœ‰æ€ç»´é“¾ç‰‡æ®µ
    has_reasoning = False  # æ ‡è®°æ˜¯å¦æœ‰æ€ç»´é“¾å†…å®¹
    reasoning_ended = False  # æ ‡è®°reasoningæ˜¯å¦å·²ç»“æŸ
    
    # è¯Šæ–­ï¼šæ·»åŠ æµå¼æ€§èƒ½è¿½è¸ª
    import time as time_module
    last_yield_time = time_module.time()
    chunk_count = 0
    total_chars = 0

    try:
        while True:
            # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥è¯·æ±‚æ˜¯å¦åº”è¯¥è¢«å–æ¶ˆ
            if request_id not in response_channels:
                logger.warning(f"[STREAM_LIFECYCLE] âš ï¸ è¯·æ±‚é€šé“å·²å…³é—­ï¼ˆå¯èƒ½å®¢æˆ·ç«¯æ–­å¼€ï¼‰: {request_id[:8]}")
                stream_cancelled = True
                
                # ğŸ”§ å‘æµè§ˆå™¨å‘é€å–æ¶ˆæŒ‡ä»¤
                if request_id in request_metadata:
                    tab_id = request_metadata[request_id].get("tab_id")
                    if tab_id and tab_id in browser_connections:
                        ws = browser_connections[tab_id]
                        cancel_payload = {
                            "command": "cancel_request",
                            "request_id": request_id
                        }
                        try:
                            asyncio.create_task(ws.send_text(json.dumps(cancel_payload)))
                            logger.warning(f"[STREAM_LIFECYCLE] âœ‰ï¸ é€šé“å…³é—­æ—¶å·²å‘æµè§ˆå™¨å‘é€å–æ¶ˆæŒ‡ä»¤: {request_id[:8]}")
                        except Exception as e:
                            logger.error(f"[STREAM_LIFECYCLE] å‘é€å–æ¶ˆæŒ‡ä»¤å¤±è´¥: {e}")
                
                break
            
            # å…³é”®ä¿®å¤ï¼šæ¯æ¬¡å¾ªç¯å¼€å§‹æ—¶é‡ç½®reasoning_foundæ ‡å¿—
            reasoning_found_in_this_chunk = False
            
            try:
                # è¯Šæ–­ï¼šè®°å½•æ¥æ”¶æ•°æ®çš„æ—¶é—´
                receive_start = time_module.time()
                raw_data = await asyncio.wait_for(queue.get(), timeout=timeout)
                receive_time = time_module.time() - receive_start
                
                if CONFIG.get("debug_stream_timing", False):
                    logger.debug(f"[STREAM_TIMING] ä»é˜Ÿåˆ—è·å–æ•°æ®è€—æ—¶: {receive_time:.3f}ç§’")
                    # è¯Šæ–­ï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®çš„å‰200ä¸ªå­—ç¬¦
                    raw_data_str = str(raw_data)[:200] if raw_data else "None"
                    logger.debug(f"[STREAM_RAW] åŸå§‹æ•°æ®: {raw_data_str}...")
                    
            except asyncio.TimeoutError:
                logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: ç­‰å¾…æµè§ˆå™¨æ•°æ®è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ã€‚")
                yield 'error', f'Response timed out after {timeout} seconds.'
                return

            # --- Cloudflare äººæœºéªŒè¯å¤„ç† ---
            def handle_cloudflare_verification():
                nonlocal IS_REFRESHING_FOR_VERIFICATION, VERIFICATION_COOLDOWN_UNTIL
                if not IS_REFRESHING_FOR_VERIFICATION:
                    logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: é¦–æ¬¡æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå°†å‘é€åˆ·æ–°æŒ‡ä»¤å¹¶å¯åŠ¨25ç§’å†·å´ã€‚")
                    IS_REFRESHING_FOR_VERIFICATION = True
                    # è®¾ç½®25ç§’å†·å´æœŸ
                    VERIFICATION_COOLDOWN_UNTIL = time.time() + 25
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦browser_wsï¼Œä½†å®ƒä¸åœ¨å‚æ•°ä¸­ï¼Œéœ€è¦ä»browser_connectionsè·å–
                    if browser_connections:
                        first_ws = list(browser_connections.values())[0]
                        asyncio.create_task(first_ws.send_text(json.dumps({"command": "refresh"}, ensure_ascii=False)))
                    
                    # å¯åŠ¨åå°ä»»åŠ¡ï¼š25ç§’åè‡ªåŠ¨é‡ç½®çŠ¶æ€
                    async def reset_verification_status():
                        await asyncio.sleep(25)
                        nonlocal IS_REFRESHING_FOR_VERIFICATION, VERIFICATION_COOLDOWN_UNTIL
                        IS_REFRESHING_FOR_VERIFICATION = False
                        VERIFICATION_COOLDOWN_UNTIL = None
                        logger.info("â° äººæœºéªŒè¯å†·å´æœŸå·²ç»“æŸï¼Œç³»ç»Ÿå·²æ¢å¤æ­£å¸¸ã€‚")
                    
                    asyncio.create_task(reset_verification_status())
                    return "æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå·²å‘é€åˆ·æ–°æŒ‡ä»¤ã€‚ç³»ç»Ÿå°†å†·å´25ç§’ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                else:
                    # è®¡ç®—å‰©ä½™å†·å´æ—¶é—´
                    if VERIFICATION_COOLDOWN_UNTIL:
                        remaining = max(0, int(VERIFICATION_COOLDOWN_UNTIL - time.time()))
                        if remaining > 0:
                            logger.info(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå†·å´ä¸­ï¼ˆå‰©ä½™{remaining}ç§’ï¼‰ã€‚")
                            return f"æ­£åœ¨ç­‰å¾…äººæœºéªŒè¯å†·å´å®Œæˆ...ï¼ˆå‰©ä½™ {remaining} ç§’ï¼‰"
                    logger.info(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œä½†å·²åœ¨åˆ·æ–°ä¸­ï¼Œå°†ç­‰å¾…ã€‚")
                    return "æ­£åœ¨ç­‰å¾…äººæœºéªŒè¯å®Œæˆ..."

            # 1. æ£€æŸ¥æ¥è‡ª WebSocket ç«¯çš„ç›´æ¥é”™è¯¯æˆ–é‡è¯•ä¿¡æ¯
            if isinstance(raw_data, dict):
                # å¤„ç†é‡è¯•ä¿¡æ¯
                if 'retry_info' in raw_data:
                    retry_info = raw_data.get('retry_info', {})
                    logger.info(f"PROCESSOR [ID: {request_id[:8]}]: æ”¶åˆ°é‡è¯•ä¿¡æ¯ - å°è¯• {retry_info.get('attempt')}/{retry_info.get('max_attempts')}")
                    # å¯ä»¥é€‰æ‹©å°†é‡è¯•ä¿¡æ¯ä¼ é€’ç»™å®¢æˆ·ç«¯
                    yield 'retry_info', retry_info
                    continue
                
                # å¤„ç†é”™è¯¯
                if 'error' in raw_data:
                    error_msg = raw_data.get('error', 'Unknown browser error')
                if isinstance(error_msg, str):
                    if '413' in error_msg or 'too large' in error_msg.lower():
                        friendly_error_msg = "ä¸Šä¼ å¤±è´¥ï¼šé™„ä»¶å¤§å°è¶…è¿‡äº† LMArena æœåŠ¡å™¨çš„é™åˆ¶ (é€šå¸¸æ˜¯ 5MBå·¦å³)ã€‚è¯·å°è¯•å‹ç¼©æ–‡ä»¶æˆ–ä¸Šä¼ æ›´å°çš„æ–‡ä»¶ã€‚"
                        logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°é™„ä»¶è¿‡å¤§é”™è¯¯ (413)ã€‚")
                        yield 'error', friendly_error_msg
                        return
                    if any(re.search(p, error_msg, re.IGNORECASE) for p in cloudflare_patterns):
                        yield 'error', handle_cloudflare_verification()
                        return
                yield 'error', error_msg
                return

            # 2. æ£€æŸ¥ [DONE] ä¿¡å·
            if raw_data == "[DONE]":
                logger.info(f"[STREAM_END] æ”¶åˆ°[DONE]ä¿¡å· - è¯·æ±‚ {request_id[:8]}")
                
                # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šç­‰å¾…é¢å¤–çš„æ•°æ®å¯èƒ½è¿˜åœ¨ä¼ è¾“ä¸­
                logger.info(f"[STREAM_END] â³ ç­‰å¾…200msä»¥æ¥æ”¶å¯èƒ½å»¶è¿Ÿåˆ°è¾¾çš„æ•°æ®...")
                try:
                    # å°è¯•æ¥æ”¶æ›´å¤šæ•°æ®ï¼Œä½†è®¾ç½®çŸ­è¶…æ—¶
                    extra_data = await asyncio.wait_for(queue.get(), timeout=0.2)
                    if extra_data != "[DONE]":
                        logger.info(f"[STREAM_END] âœ… æ”¶åˆ°å»¶è¿Ÿæ•°æ®ï¼Œé•¿åº¦: {len(str(extra_data))}")
                        # å°†å»¶è¿Ÿæ•°æ®æ·»åŠ åˆ°buffer
                        buffer += "".join(str(item) for item in extra_data) if isinstance(extra_data, list) else extra_data
                except asyncio.TimeoutError:
                    logger.info(f"[STREAM_END] â° è¶…æ—¶ï¼Œæ²¡æœ‰æ›´å¤šå»¶è¿Ÿæ•°æ®")
                
                # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šåœ¨é€€å‡ºå‰å¼ºåˆ¶å¤„ç†bufferä¸­çš„æ‰€æœ‰å‰©ä½™å†…å®¹
                if len(buffer) > 0:
                    logger.info(f"[STREAM_END] âš ï¸ Bufferè¿˜æœ‰ {len(buffer)} å­—ç¬¦æœªå¤„ç†ï¼Œå¼€å§‹å¼ºåˆ¶æå–...")
                    logger.debug(f"[STREAM_END] Bufferå®Œæ•´å†…å®¹: {buffer}")
                    
                    final_extracted_count = 0
                    
                    # ğŸ”§ æ”¹è¿›ï¼šä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™æ¥åŒ¹é…å¯èƒ½è¢«æˆªæ–­çš„å†…å®¹
                    extraction_attempts = 0
                    max_attempts = 100  # é˜²æ­¢æ— é™å¾ªç¯
                    
                    while extraction_attempts < max_attempts:
                        extraction_attempts += 1
                        match = text_pattern.search(buffer)
                        
                        if not match:
                            # å¦‚æœæ²¡æœ‰å®Œæ•´åŒ¹é…ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½è¢«æˆªæ–­çš„å†…å®¹
                            partial_pattern = re.compile(r'[ab]0:"([^"]*?)(?:"|$)')
                            partial_match = partial_pattern.search(buffer)
                            
                            if partial_match and len(partial_match.group(1)) > 0:
                                logger.warning(f"[STREAM_END] å‘ç°å¯èƒ½è¢«æˆªæ–­çš„å†…å®¹ï¼Œå°è¯•æå–...")
                                matched_text = partial_match.group(1)
                                match_end = partial_match.end()
                            else:
                                break  # æ²¡æœ‰æ›´å¤šå¯æå–çš„å†…å®¹
                        else:
                            matched_text = match.group(1)
                            match_end = match.end()
                        
                        try:
                            text_content = json.loads(f'"{matched_text}"')
                            if text_content:
                                has_yielded_content = True
                                total_chars += len(text_content)
                                final_extracted_count += 1
                                
                                logger.info(f"[STREAM_END] æå–æ–‡æœ¬å—#{final_extracted_count}: {text_content[:100]}...")
                                yield 'content', text_content
                                
                                # ç«‹å³å¤„ç†ï¼Œé¿å…é˜»å¡
                                await asyncio.sleep(0)
                        except (ValueError, json.JSONDecodeError) as e:
                            logger.warning(f"[STREAM_END] JSONè§£æå¤±è´¥: {e}, æ–‡æœ¬: {matched_text[:100]}")
                        
                        # åˆ é™¤å·²å¤„ç†çš„éƒ¨åˆ†
                        buffer = buffer[match_end:]
                    
                    # å¤„ç†å‰©ä½™çš„æ€ç»´é“¾å†…å®¹
                    while (match := reasoning_pattern.search(buffer)):
                        try:
                            reasoning_content = json.loads(f'"{match.group(1)}"')
                            if reasoning_content and enable_reasoning_output:
                                has_reasoning = True
                                reasoning_buffer.append(reasoning_content)
                                if CONFIG.get("preserve_streaming", True):
                                    yield 'reasoning', reasoning_content
                        except (ValueError, json.JSONDecodeError):
                            pass
                        buffer = buffer[match.end():]
                    
                    # å¤„ç†å‰©ä½™çš„å›¾ç‰‡å†…å®¹
                    while (match := image_pattern.search(buffer)):
                        try:
                            image_data_list = json.loads(match.group(1))
                            if isinstance(image_data_list, list) and image_data_list:
                                image_info = image_data_list[0]
                                if image_info.get("type") == "image" and "image" in image_info:
                                    # è¿™é‡Œåº”è¯¥ç»§ç»­å®Œæ•´çš„å›¾ç‰‡å¤„ç†é€»è¾‘
                                    # ä½†ç”±äºä»£ç å¤ªé•¿ï¼Œæš‚æ—¶è·³è¿‡
                                    pass
                        except (json.JSONDecodeError, IndexError):
                            pass
                        buffer = buffer[match.end():]
                    
                    if final_extracted_count > 0:
                        logger.info(f"[STREAM_END] âœ… æˆåŠŸä»bufferæå–äº† {final_extracted_count} ä¸ªæ–‡æœ¬å—ï¼Œå…± {total_chars} å­—ç¬¦")
                    elif len(buffer) > 0:
                        logger.warning(f"[STREAM_END] âš ï¸ æœªèƒ½ä»bufferæå–ä»»ä½•å†…å®¹ï¼Œbufferå¯èƒ½æ ¼å¼å¼‚å¸¸")
                        logger.warning(f"[STREAM_END] å‰©ä½™bufferé•¿åº¦: {len(buffer)}")
                        logger.warning(f"[STREAM_END] å‰©ä½™bufferå†…å®¹: {buffer}")
                        
                        # ğŸ”§ æ£€æŸ¥æ˜¯å¦æ˜¯éå†…å®¹æ ‡è®°
                        is_control_marker = False
                        for control_prefix in ['a3:', 'ad:', 'b3:', 'bd:', 'ae:', 'be:']:
                            if control_prefix in buffer:
                                logger.info(f"[STREAM_END] æ£€æµ‹åˆ°æ§åˆ¶æ ‡è®° {control_prefix}ï¼Œå¿½ç•¥ä¸è¾“å‡º")
                                is_control_marker = True
                                break
                        
                        # åªæœ‰åœ¨ä¸æ˜¯æ§åˆ¶æ ‡è®°æ—¶æ‰å°è¯•ä½œä¸ºæ–‡æœ¬è¾“å‡º
                        if not is_control_marker and buffer.strip() and not buffer.startswith('[') and not buffer.startswith('{'):
                            logger.warning(f"[STREAM_END] å°è¯•å°†å‰©ä½™å†…å®¹ä½œä¸ºæ™®é€šæ–‡æœ¬å¤„ç†...")
                            # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼Œåªä¿ç•™å¯æ‰“å°å­—ç¬¦
                            clean_text = ''.join(c for c in buffer if c.isprintable() or c in '\n\r\t')
                            if clean_text.strip():
                                logger.info(f"[STREAM_END] âš¡ ä»å¼‚å¸¸bufferä¸­æå–åˆ°æ–‡æœ¬: {clean_text[:200]}...")
                                yield 'content', clean_text
                
                if has_yielded_content and IS_REFRESHING_FOR_VERIFICATION:
                     logger.info(f"PROCESSOR [ID: {request_id[:8]}]: è¯·æ±‚æˆåŠŸï¼ŒäººæœºéªŒè¯çŠ¶æ€å°†åœ¨ä¸‹æ¬¡è¿æ¥æ—¶é‡ç½®ã€‚")
                break

            # 3. ç´¯åŠ ç¼“å†²åŒºå¹¶æ£€æŸ¥å†…å®¹
            buffer += "".join(str(item) for item in raw_data) if isinstance(raw_data, list) else raw_data
            
            # è¯Šæ–­ï¼šæ˜¾ç¤ºç¼“å†²åŒºå¤§å°
            if CONFIG.get("debug_stream_timing", False):
                logger.debug(f"[STREAM_BUFFER] ç¼“å†²åŒºå¤§å°: {len(buffer)} å­—ç¬¦")

            if any(re.search(p, buffer, re.IGNORECASE) for p in cloudflare_patterns):
                yield 'error', handle_cloudflare_verification()
                return
            
            if (error_match := error_pattern.search(buffer)):
                try:
                    error_json = json.loads(error_match.group(1))
                    yield 'error', error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    return
                except json.JSONDecodeError: pass

            # ä¼˜å…ˆå¤„ç†æ€ç»´é“¾å†…å®¹ï¼ˆagå‰ç¼€ï¼‰
            reasoning_found_in_this_chunk = False
            while (match := reasoning_pattern.search(buffer)):
                try:
                    reasoning_content = json.loads(f'"{match.group(1)}"')
                    if reasoning_content:
                        # è­¦å‘Šï¼šæ£€æµ‹åˆ°reasoningåœ¨contentä¹‹åå‡ºç°ï¼ˆå¼‚å¸¸æƒ…å†µï¼‰
                        if reasoning_ended:
                            logger.warning(f"[REASONING_WARN] æ£€æµ‹åˆ°reasoningåœ¨contentä¹‹åç»§ç»­å‡ºç°ï¼Œè¿™å¯èƒ½å¯¼è‡´think_tagæ¨¡å¼ä¸‹å†…å®¹ä¸¢å¤±ï¼")
                        
                        # æ€»æ˜¯æ”¶é›†æ€ç»´é“¾ï¼ˆç”¨äºç›‘æ§å’Œæ—¥å¿—ï¼‰
                        has_reasoning = True
                        reasoning_buffer.append(reasoning_content)
                        reasoning_found_in_this_chunk = True
                        
                        # åªåœ¨é…ç½®å¯ç”¨æ—¶æ‰è¾“å‡ºç»™å®¢æˆ·ç«¯
                        if enable_reasoning_output and CONFIG.get("preserve_streaming", True):
                            # æµå¼è¾“å‡ºæ€ç»´é“¾
                            yield 'reasoning', reasoning_content
                        
                except (ValueError, json.JSONDecodeError) as e:
                    if CONFIG.get("debug_stream_timing", False):
                        logger.debug(f"[REASONING_ERROR] è§£æé”™è¯¯: {e}")
                    pass
                buffer = buffer[match.end():]
            
            # å¤„ç†æ–‡æœ¬å†…å®¹ï¼ˆa0å‰ç¼€ï¼‰- æ·»åŠ è¯Šæ–­
            process_start = time_module.time()
            chunks_in_buffer = 0
            
            # è¯Šæ–­ï¼šæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…
            if CONFIG.get("debug_stream_timing", False):
                matches_found = text_pattern.findall(buffer)
                if matches_found:
                    logger.debug(f"[STREAM_MATCH] æ‰¾åˆ° {len(matches_found)} ä¸ªæ–‡æœ¬åŒ¹é…")
                    for idx, match in enumerate(matches_found[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        logger.debug(f"  åŒ¹é…#{idx+1}: {match[:50]}...")
            
            while (match := text_pattern.search(buffer)):
                matched_text = match.group(1)
                match_end = match.end()
                
                try:
                    text_content = json.loads(f'"{matched_text}"')
                    if text_content:
                        # å…³é”®ä¿®å¤ï¼šåœ¨ç¬¬ä¸€ä¸ªcontentåˆ°æ¥æ—¶ï¼Œå¦‚æœæœ‰reasoningä¸”æœªç»“æŸï¼Œåˆ™æ ‡è®°ç»“æŸ
                        if has_reasoning and not reasoning_ended and not reasoning_found_in_this_chunk:
                            reasoning_ended = True
                            logger.info(f"[REASONING_END] æ£€æµ‹åˆ°reasoningç»“æŸï¼ˆå…±{len(reasoning_buffer)}ä¸ªç‰‡æ®µï¼‰")
                            # åªåœ¨å¯ç”¨è¾“å‡ºæ—¶æ‰å‘é€ç»“æŸäº‹ä»¶
                            if enable_reasoning_output:
                                yield 'reasoning_end', None
                        
                        has_yielded_content = True
                        chunk_count += 1
                        total_chars += len(text_content)
                        chunks_in_buffer += 1
                        
                        # è¯Šæ–­ï¼šè®°å½•yieldé—´éš”
                        current_time = time_module.time()
                        yield_interval = current_time - last_yield_time
                        last_yield_time = current_time
                        
                        if CONFIG.get("debug_stream_timing", False):
                            logger.debug(f"[STREAM_TIMING] Yieldé—´éš”: {yield_interval:.3f}ç§’, "
                                       f"å—#{chunk_count}, å­—ç¬¦æ•°: {len(text_content)}, "
                                       f"ç´¯è®¡å­—ç¬¦: {total_chars}")
                        
                        yield 'content', text_content
                        
                        # ç«‹å³å¤„ç†ï¼Œä¸è¦ç­‰å¾…
                        await asyncio.sleep(0)
                        
                        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæˆåŠŸå¤„ç†åæ‰åˆ é™¤buffer
                        buffer = buffer[match_end:]
                    else:
                        # ç©ºå†…å®¹ï¼Œä¹Ÿè¦åˆ é™¤ä»¥é¿å…æ­»å¾ªç¯
                        buffer = buffer[match_end:]
                        
                except (ValueError, json.JSONDecodeError) as e:
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šè§£æå¤±è´¥æ—¶è®°å½•é”™è¯¯ä½†ä»ç„¶åˆ é™¤ï¼Œé¿å…æ­»å¾ªç¯
                    logger.warning(f"[PARSE_ERROR] JSONè§£æå¤±è´¥: {e}, åŒ¹é…æ–‡æœ¬: {matched_text[:100]}...")
                    buffer = buffer[match_end:]
            
            # è¯Šæ–­ï¼šè®°å½•å¤„ç†æ—¶é—´
            if chunks_in_buffer > 0 and CONFIG.get("debug_stream_timing", False):
                process_time = time_module.time() - process_start
                logger.debug(f"[STREAM_TIMING] å¤„ç†{chunks_in_buffer}ä¸ªæ–‡æœ¬å—è€—æ—¶: {process_time:.3f}ç§’")

            # æ–°å¢ï¼šå¤„ç†å›¾ç‰‡å†…å®¹ï¼ˆç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            while (match := image_pattern.search(buffer)):
                try:
                    image_data_list = json.loads(match.group(1))
                    if isinstance(image_data_list, list) and image_data_list:
                        image_info = image_data_list[0]
                        if image_info.get("type") == "image" and "image" in image_info:
                            image_url = image_info['image']
                            
                            # å°†LMArenaè¿”å›çš„å›¾ç‰‡URLè½¬æ¢ä¸ºbase64è¿”å›ç»™å®¢æˆ·ç«¯
                            show_full_urls = CONFIG.get("debug_show_full_urls", False)
                            if show_full_urls:
                                logger.info(f"ğŸ“¥ LMArenaè¿”å›å›¾ç‰‡URLï¼ˆå®Œæ•´ï¼‰: {image_url}")
                            else:
                                display_length = CONFIG.get("url_display_length", 200)
                                if len(image_url) <= display_length:
                                    logger.info(f"ğŸ“¥ LMArenaè¿”å›å›¾ç‰‡URL: {image_url}")
                                else:
                                    logger.info(f"ğŸ“¥ LMArenaè¿”å›å›¾ç‰‡URL: {image_url[:display_length]}...")
                                    logger.debug(f"   å®Œæ•´URL: {image_url}")
                            
                            # è®°å½•å¼€å§‹æ—¶é—´
                            import time as time_module
                            process_start_time = time_module.time()
                            
                            # è·å–è¿”å›æ¨¡å¼é…ç½®
                            return_format_config = CONFIG.get("image_return_format", {})
                            return_mode = return_format_config.get("mode", "base64")
                            save_locally = CONFIG.get("save_images_locally", True)
                            
                            logger.info(f"[IMG_PROCESS] å¼€å§‹å¤„ç†å›¾ç‰‡")
                            logger.info(f"  - è¿”å›æ¨¡å¼: {return_mode}")
                            logger.info(f"  - æœ¬åœ°ä¿å­˜: {save_locally}")
                            
                            # URLæ¨¡å¼ï¼šç«‹å³è¿”å›ï¼Œä¸é˜»å¡
                            if return_mode == "url":
                                logger.info(f"[IMG_PROCESS] URLæ¨¡å¼ - ç«‹å³è¿”å›URLç»™å®¢æˆ·ç«¯")
                                yield 'content', f"![Image]({image_url})"
                                
                                # å¦‚æœéœ€è¦ä¿å­˜åˆ°æœ¬åœ°ï¼Œåˆ›å»ºåå°ä»»åŠ¡ï¼ˆä¸é˜»å¡å“åº”ï¼‰
                                if save_locally:
                                    logger.info(f"[IMG_PROCESS] å¯åŠ¨åå°ä»»åŠ¡å¼‚æ­¥ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡")
                                    
                                    async def async_download_and_save():
                                        try:
                                            download_start = time_module.time()
                                            img_data, err = await _download_image_data_with_retry(image_url)
                                            download_time = time_module.time() - download_start
                                            
                                            if img_data:
                                                logger.info(f"[IMG_PROCESS] åå°ä¸‹è½½æˆåŠŸï¼Œè€—æ—¶: {download_time:.2f}ç§’")
                                                await save_downloaded_image_async(img_data, image_url, request_id)
                                                logger.info(f"[IMG_PROCESS] å›¾ç‰‡å·²ä¿å­˜åˆ°æœ¬åœ°")
                                            else:
                                                logger.error(f"[IMG_PROCESS] åå°ä¸‹è½½å¤±è´¥: {err}")
                                        except Exception as e:
                                            logger.error(f"[IMG_PROCESS] åå°ä»»åŠ¡å¼‚å¸¸: {e}")
                                    
                                    asyncio.create_task(async_download_and_save())
                                else:
                                    logger.info(f"[IMG_PROCESS] save_images_locally=falseï¼Œè·³è¿‡ä¸‹è½½")
                                
                                # URLæ¨¡å¼å¤„ç†å®Œæˆï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ¶ˆæ¯
                                continue
                            
                            # Base64æ¨¡å¼ï¼šå¿…é¡»å…ˆä¸‹è½½æ‰èƒ½è½¬æ¢
                            logger.info(f"[IMG_PROCESS] Base64æ¨¡å¼ - éœ€è¦ä¸‹è½½å›¾ç‰‡è¿›è¡Œè½¬æ¢")
                            
                            # ä¸‹è½½å›¾ç‰‡æ•°æ®
                            download_start_time = time_module.time()
                            image_data, download_error = await _download_image_data_with_retry(image_url)
                            download_time = time_module.time() - download_start_time
                            logger.info(f"[IMG_PROCESS] å›¾ç‰‡ä¸‹è½½å®Œæˆï¼Œè€—æ—¶: {download_time:.2f}ç§’")
                            
                            # å¦‚æœéœ€è¦ä¿å­˜åˆ°æœ¬åœ°
                            if save_locally and image_data:
                                logger.info(f"[IMG_PROCESS] å¼‚æ­¥ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°")
                                asyncio.create_task(save_downloaded_image_async(image_data, image_url, request_id))
                            elif not save_locally:
                                logger.info(f"[IMG_PROCESS] save_images_locally=falseï¼Œè·³è¿‡æœ¬åœ°ä¿å­˜")
                            
                            # Base64è½¬æ¢
                            if True:  # è¿™é‡Œç¡®å®šæ˜¯base64æ¨¡å¼
                                if image_data:
                                    # --- Base64 è½¬æ¢å’Œç¼“å­˜é€»è¾‘ ---
                                    cache_key = image_url
                                    current_time = time_module.time()
                                    
                                    # æ¸…ç†è¿‡æœŸç¼“å­˜
                                    if len(IMAGE_BASE64_CACHE) > IMAGE_CACHE_MAX_SIZE:
                                        sorted_items = sorted(IMAGE_BASE64_CACHE.items(), key=lambda x: x[1][1])
                                        for url, _ in sorted_items[:IMAGE_CACHE_MAX_SIZE // 2]:
                                            del IMAGE_BASE64_CACHE[url]
                                        logger.info(f"  ğŸ§¹ æ¸…ç†äº† {IMAGE_CACHE_MAX_SIZE // 2} ä¸ªæ—§ç¼“å­˜")

                                    # æ£€æŸ¥ç¼“å­˜
                                    if cache_key in IMAGE_BASE64_CACHE:
                                        cached_data, cache_time = IMAGE_BASE64_CACHE[cache_key]
                                        if current_time - cache_time < IMAGE_CACHE_TTL:
                                            logger.info(f"  âš¡ ä»ç¼“å­˜è·å–å›¾ç‰‡Base64")
                                            yield 'content', cached_data
                                            continue
                                    
                                    # æ‰§è¡Œè½¬æ¢
                                    content_type = mimetypes.guess_type(image_url)[0] or 'image/png'
                                    image_base64 = base64.b64encode(image_data).decode('ascii')
                                    data_url = f"data:{content_type};base64,{image_base64}"
                                    markdown_image = f"![Image]({data_url})"
                                    
                                    # å­˜å…¥ç¼“å­˜
                                    IMAGE_BASE64_CACHE[cache_key] = (markdown_image, current_time)
                                    
                                    # è®¡ç®—æ€»è€—æ—¶
                                    total_time = time_module.time() - process_start_time
                                    logger.info(f"[IMG_PROCESS] Base64è½¬æ¢å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
                                    
                                    yield 'content', markdown_image
                                else:
                                    # ä¸‹è½½å¤±è´¥ï¼Œé™çº§è¿”å›URL
                                    logger.error(f"[IMG_PROCESS] âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ ({download_error})ï¼Œé™çº§è¿”å›åŸå§‹URL")
                                    total_time = time_module.time() - process_start_time
                                    logger.info(f"[IMG_PROCESS] å¤„ç†å®Œæˆï¼ˆå¤±è´¥é™çº§ï¼‰ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
                                    yield 'content', f"![Image]({image_url})"

                except (json.JSONDecodeError, IndexError) as e:
                    logger.warning(f"è§£æå›¾ç‰‡URLæ—¶å‡ºé”™: {e}, buffer: {buffer[:150]}")
                buffer = buffer[match.end():]

            if (finish_match := finish_pattern.search(buffer)):
                try:
                    finish_data = json.loads(finish_match.group(1))
                    finish_reason = finish_data.get("finishReason", "stop")
                    
                    # ğŸ”§ æ–°å¢ï¼šå°è¯•æå–LMArenaè¿”å›çš„å®é™…tokenä½¿ç”¨ä¿¡æ¯
                    usage_info = None
                    if "usage" in finish_data:
                        usage_info = finish_data["usage"]
                        logger.info(f"[TOKEN_EXTRACT] ä»LMArenaæå–åˆ°tokenä½¿ç”¨ä¿¡æ¯: {usage_info}")
                    elif "tokenUsage" in finish_data:
                        usage_info = finish_data["tokenUsage"]
                        logger.info(f"[TOKEN_EXTRACT] ä»LMArenaæå–åˆ°tokenUsageä¿¡æ¯: {usage_info}")
                    
                    # å°†finish_reasonå’Œusage_infoä¸€èµ·ä¼ é€’
                    yield 'finish', {'reason': finish_reason, 'usage': usage_info}
                except (json.JSONDecodeError, IndexError): pass
                buffer = buffer[finish_match.end():]

    except asyncio.CancelledError:
        stream_cancelled = True
        logger.warning(f"[STREAM_LIFECYCLE] ğŸš« ä»»åŠ¡è¢«å–æ¶ˆï¼ˆasyncio.CancelledErrorï¼‰: {request_id[:8]}")
        logger.warning(f"  - è¿™æ„å‘³ç€å®¢æˆ·ç«¯å·²æ–­å¼€ï¼Œåº”è¯¥åœæ­¢å¤„ç†")
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå‘æµè§ˆå™¨å‘é€å–æ¶ˆæŒ‡ä»¤ï¼Œä¸­æ­¢fetchè¯·æ±‚
        if request_id in request_metadata:
            tab_id = request_metadata[request_id].get("tab_id")
            if tab_id and tab_id in browser_connections:
                ws = browser_connections[tab_id]
                cancel_payload = {
                    "command": "cancel_request",
                    "request_id": request_id
                }
                try:
                    # ä½¿ç”¨ create_task é¿å…é˜»å¡æ¸…ç†æµç¨‹
                    asyncio.create_task(ws.send_text(json.dumps(cancel_payload)))
                    logger.warning(f"[STREAM_LIFECYCLE] âœ‰ï¸ å·²å‘æµè§ˆå™¨å‘é€å–æ¶ˆæŒ‡ä»¤: {request_id[:8]}")
                except Exception as e:
                    logger.error(f"[STREAM_LIFECYCLE] å‘é€å–æ¶ˆæŒ‡ä»¤å¤±è´¥: {e}")
            else:
                logger.warning(f"[STREAM_LIFECYCLE] âš ï¸ æ— æ³•å‘é€å–æ¶ˆæŒ‡ä»¤ï¼šæ‰¾ä¸åˆ°æ ‡ç­¾é¡µè¿æ¥")
        else:
            logger.warning(f"[STREAM_LIFECYCLE] âš ï¸ æ— æ³•å‘é€å–æ¶ˆæŒ‡ä»¤ï¼šæ‰¾ä¸åˆ°è¯·æ±‚å…ƒæ•°æ®")
    finally:
        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè®°å½•æµå¤„ç†ç»“æŸçŠ¶æ€
        if stream_cancelled:
            logger.warning(f"[STREAM_LIFECYCLE] â›” æµå¤„ç†å¼‚å¸¸ç»“æŸ: {request_id[:8]}")
            logger.warning(f"  - åŸå› : ä»»åŠ¡å–æ¶ˆæˆ–é€šé“å…³é—­")
        else:
            logger.info(f"[STREAM_LIFECYCLE] âœ… æµå¤„ç†æ­£å¸¸ç»“æŸ: {request_id[:8]}")
        
        # åœ¨æ¸…ç†å‰ï¼Œå¦‚æœæœ‰æ€ç»´é“¾å†…å®¹ä¸”æœªæµå¼è¾“å‡ºï¼Œåˆ™ä¸€æ¬¡æ€§è¾“å‡º
        if enable_reasoning_output and has_reasoning and not CONFIG.get("preserve_streaming", True):
            # éæµå¼æ¨¡å¼ï¼šåœ¨æœ€åä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´æ€ç»´é“¾
            full_reasoning = "".join(reasoning_buffer)
            yield 'reasoning_complete', full_reasoning
        
        # è¯Šæ–­ï¼šè¾“å‡ºæµå¼æ€§èƒ½ç»Ÿè®¡
        if chunk_count > 0 and CONFIG.get("debug_stream_timing", False):
            total_time = time_module.time() - (last_yield_time - yield_interval if 'yield_interval' in locals() else last_yield_time)
            logger.info(f"[STREAM_STATS] è¯·æ±‚ID: {request_id[:8]}")
            logger.info(f"  - æ€»å—æ•°: {chunk_count}")
            logger.info(f"  - æ€»å­—ç¬¦æ•°: {total_chars}")
            logger.info(f"  - å¹³å‡å—å¤§å°: {total_chars/chunk_count:.1f}å­—ç¬¦")
            logger.info(f"  - å¹³å‡yieldé—´éš”: {total_time/chunk_count:.3f}ç§’")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šé‡Šæ”¾æ ‡ç­¾é¡µè¯·æ±‚è®¡æ•°
        if request_id in request_metadata:
            tab_id = request_metadata[request_id].get("tab_id")
            if tab_id:
                await release_tab_request(tab_id)
                logger.debug(f"PROCESSOR [ID: {request_id[:8]}]: å·²é‡Šæ”¾æ ‡ç­¾é¡µ '{tab_id}' çš„è¯·æ±‚è®¡æ•°")
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå»¶è¿Ÿæ¸…ç†å“åº”é€šé“ï¼Œç»™æµè§ˆå™¨ç¼“å†²åŒºæ—¶é—´å‘é€æœ€åçš„æ•°æ®
        # åªåœ¨æ­£å¸¸ç»“æŸæ—¶å»¶è¿Ÿï¼Œå–æ¶ˆæ—¶ç«‹å³æ¸…ç†
        if not stream_cancelled:
            logger.debug(f"[STREAM_LIFECYCLE] ç­‰å¾…1ç§’åæ¸…ç†é€šé“ï¼Œç¡®ä¿æµè§ˆå™¨ç¼“å†²æ•°æ®å‘é€å®Œæ¯•")
            await asyncio.sleep(1.0)
        
        if request_id in response_channels:
            del response_channels[request_id]
            logger.info(f"PROCESSOR [ID: {request_id[:8]}]: å“åº”é€šé“å·²æ¸…ç†ã€‚")
        
        # æ¸…ç†è¯·æ±‚å…ƒæ•°æ®ï¼ˆä¿®å¤å†…å­˜æ³„æ¼ï¼‰
        if request_id in request_metadata:
            del request_metadata[request_id]
            logger.debug(f"PROCESSOR [ID: {request_id[:8]}]: è¯·æ±‚å…ƒæ•°æ®å·²æ¸…ç†ã€‚")


async def stream_generator(request_id: str, model: str, _process_lmarena_stream_func,
                           format_openai_chunk, format_openai_finish_chunk, format_openai_error_chunk,
                           CONFIG: dict, response_channels: dict, request_metadata: dict,
                           monitoring_service, estimate_message_tokens, estimate_tokens,
                           browser_connections: dict):
    """å°†å†…éƒ¨äº‹ä»¶æµæ ¼å¼åŒ–ä¸º OpenAI SSE å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨å¯åŠ¨ã€‚")
    
    # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šæ ‡è®°ç”Ÿæˆå™¨æ˜¯å¦è¢«å–æ¶ˆ
    is_cancelled = False
    client_disconnected = False
    
    finish_reason_to_send = 'stop'  # é»˜è®¤çš„ç»“æŸåŸå› 
    collected_content = []  # æ”¶é›†å“åº”å†…å®¹ç”¨äºå­˜å‚¨
    reasoning_content = []  # æ”¶é›†æ€ç»´é“¾å†…å®¹
    lmarena_usage = None  # å­˜å‚¨ä»LMArenaæå–çš„tokenä½¿ç”¨ä¿¡æ¯
    
    # è¯Šæ–­ï¼šæ·»åŠ æµå¼æ€§èƒ½è¿½è¸ª
    import time as time_module
    stream_start_time = time_module.time()
    chunks_sent = 0
    
    # æ€ç»´é“¾é…ç½®
    enable_reasoning_output = CONFIG.get("enable_lmarena_reasoning", False)
    reasoning_mode = CONFIG.get("reasoning_output_mode", "openai")
    preserve_streaming = CONFIG.get("preserve_streaming", True)

    async for event_type, data in _process_lmarena_stream_func(request_id):
        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šæ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
        try:
            # å°è¯•yieldä¸€ä¸ªç©ºæ•°æ®æ¥æ£€æµ‹å®¢æˆ·ç«¯è¿æ¥
            yield ""
        except (GeneratorExit, asyncio.CancelledError) as e:
            client_disconnected = True
            logger.warning(f"[DISCONNECT_DETECT] ğŸš« å®¢æˆ·ç«¯å·²æ–­å¼€ï¼è¯·æ±‚ {request_id[:8]}")
            logger.warning(f"  - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            
            # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šé€šçŸ¥ç›‘æ§ç³»ç»Ÿè¯·æ±‚å·²ç»“æŸ
            monitoring_service.request_end(
                request_id,
                success=False,
                error=f"Client disconnected: {type(e).__name__}"
            )
            logger.info(f"[DISCONNECT_DETECT] å·²é€šçŸ¥ç›‘æ§ç³»ç»Ÿå®¢æˆ·ç«¯æ–­å¼€: {request_id[:8]}")
            
            # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šå‘æµè§ˆå™¨å‘é€å–æ¶ˆæŒ‡ä»¤
            if request_id in request_metadata:
                tab_id = request_metadata[request_id].get("tab_id")
                if tab_id and tab_id in browser_connections:
                    ws = browser_connections[tab_id]
                    cancel_payload = {
                        "command": "cancel_request",
                        "request_id": request_id
                    }
                    # ä½¿ç”¨ create_task ä»¥å…é˜»å¡å½“å‰æ¸…ç†æµç¨‹
                    asyncio.create_task(ws.send_text(json.dumps(cancel_payload)))
                    logger.info(f"[DISCONNECT_DETECT] âœ‰ï¸  å·²å‘æ ‡ç­¾é¡µ '{tab_id}' å‘é€è¯·æ±‚å–æ¶ˆæŒ‡ä»¤: {request_id[:8]}")
                else:
                    logger.warning(f"[DISCONNECT_DETECT] âš ï¸ æ— æ³•å‘é€å–æ¶ˆæŒ‡ä»¤ï¼šæ‰¾ä¸åˆ°æ ‡ç­¾é¡µè¿æ¥")
            else:
                logger.warning(f"[DISCONNECT_DETECT] âš ï¸ æ— æ³•å‘é€å–æ¶ˆæŒ‡ä»¤ï¼šæ‰¾ä¸åˆ°è¯·æ±‚å…ƒæ•°æ® for {request_id[:8]}")

            is_cancelled = True
            break
        
        if event_type == 'retry_info':
            # å¤„ç†é‡è¯•ä¿¡æ¯
            retry_msg = f"\n[é‡è¯•ä¿¡æ¯] å°è¯• {data.get('attempt')}/{data.get('max_attempts')}ï¼ŒåŸå› : {data.get('reason')}ï¼Œç­‰å¾… {data.get('delay')/1000}ç§’...\n"
            logger.info(f"STREAMER [ID: {request_id[:8]}]: {retry_msg.strip()}")
            # å¯é€‰ï¼šå°†é‡è¯•ä¿¡æ¯ä½œä¸ºæ³¨é‡Šå‘é€ç»™å®¢æˆ·ç«¯
            if CONFIG.get("show_retry_info_to_client", False):
                yield format_openai_chunk(retry_msg, model, response_id)
        elif event_type == 'reasoning':
            # å¤„ç†æ€ç»´é“¾ç‰‡æ®µ
            reasoning_content.append(data)
            
            if enable_reasoning_output:
                if reasoning_mode == "openai" and preserve_streaming:
                    # OpenAIæ¨¡å¼ä¸”å¯ç”¨æµå¼ï¼šå‘é€reasoning delta
                    chunk = {
                        "id": response_id,
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"reasoning_content": data},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                    
        elif event_type == 'reasoning_end':
            # æ–°å¢ï¼šreasoningç»“æŸäº‹ä»¶ï¼ˆthink_tagæ¨¡å¼ä¸“ç”¨ï¼‰
            if enable_reasoning_output and reasoning_mode == "think_tag" and reasoning_content:
                # ç«‹å³è¾“å‡ºå®Œæ•´çš„reasoning
                full_reasoning = "".join(reasoning_content)
                wrapped_reasoning = f"<think>{full_reasoning}</think>\n\n"
                yield format_openai_chunk(wrapped_reasoning, model, response_id)
                logger.info(f"[THINK_TAG] å·²è¾“å‡ºå®Œæ•´reasoningï¼ˆ{len(reasoning_content)}ä¸ªç‰‡æ®µï¼‰")
                
        elif event_type == 'reasoning_complete':
            # å¤„ç†å®Œæ•´æ€ç»´é“¾ï¼ˆéæµå¼æ¨¡å¼ï¼‰
            full_reasoning = data
            reasoning_content.append(full_reasoning)
            
            if enable_reasoning_output and not preserve_streaming:
                if reasoning_mode == "openai":
                    # OpenAIæ¨¡å¼ï¼šå‘é€å®Œæ•´reasoning
                    chunk = {
                        "id": response_id,
                        "object": "chat.completion.chunk",
                        "created": int(time.time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "delta": {"reasoning_content": full_reasoning},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                elif reasoning_mode == "think_tag":
                    # think_tagæ¨¡å¼ï¼šåŒ…è£¹åä½œä¸ºcontentè¾“å‡º
                    wrapped_reasoning = f"<think>{full_reasoning}</think>\n\n"
                    yield format_openai_chunk(wrapped_reasoning, model, response_id)
                    
        elif event_type == 'content':
            
            collected_content.append(data)  # æ”¶é›†å†…å®¹
            chunks_sent += 1
            
            # ç«‹å³ç”Ÿæˆå¹¶å‘é€æ•°æ®å—ï¼Œä¸è¦ç´¯ç§¯
            chunk_data = format_openai_chunk(data, model, response_id)
            
            if CONFIG.get("debug_stream_timing", False):
                logger.debug(f"[STREAM_OUTPUT] å‘é€å—#{chunks_sent}, å¤§å°: {len(chunk_data)}å­—èŠ‚, å†…å®¹: {data[:100]}...")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ¯ä¸ªå—éƒ½è¢«å®Œæ•´yield
            try:
                yield chunk_data
            except (GeneratorExit, asyncio.CancelledError):
                # å®¢æˆ·ç«¯æ–­å¼€ï¼Œä½†ä»è¦è®°å½•å·²å‘é€çš„å†…å®¹
                logger.warning(f"[STREAM_OUTPUT] âš ï¸ å‘é€å—#{chunks_sent}æ—¶å®¢æˆ·ç«¯æ–­å¼€")
                raise
            
            # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼ˆç»™å…¶ä»–åç¨‹æ‰§è¡Œæœºä¼šï¼‰
            await asyncio.sleep(0)
        elif event_type == 'finish':
            # è®°å½•ç»“æŸåŸå› ï¼Œä½†ä¸è¦ç«‹å³è¿”å›ï¼Œç­‰å¾…æµè§ˆå™¨å‘é€ [DONE]
            # dataç°åœ¨æ˜¯ä¸€ä¸ªå­—å…¸: {'reason': ..., 'usage': ...}
            if isinstance(data, dict):
                finish_reason_to_send = data.get('reason', 'stop')
                lmarena_usage = data.get('usage')
                if lmarena_usage:
                    logger.info(f"[TOKEN_STREAM] æ•è·åˆ°LMArena tokenä½¿ç”¨ä¿¡æ¯: {lmarena_usage}")
            else:
                # å‘åå…¼å®¹æ—§æ ¼å¼
                finish_reason_to_send = data
            
            if finish_reason_to_send == 'content-filter':
                warning_msg = "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› "
                collected_content.append(warning_msg)  # ä¹Ÿæ”¶é›†è­¦å‘Šä¿¡æ¯
                yield format_openai_chunk(warning_msg, model, response_id)
        elif event_type == 'error':
            logger.error(f"STREAMER [ID: {request_id[:8]}]: æµä¸­å‘ç”Ÿé”™è¯¯: {data}")
            
            # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè®°å½•é”™è¯¯æ—¶çš„çŠ¶æ€
            logger.error(f"[DISCONNECT_DETECT] é”™è¯¯å‘ç”Ÿæ—¶çŠ¶æ€:")
            logger.error(f"  - å®¢æˆ·ç«¯æ–­å¼€: {client_disconnected}")
            logger.error(f"  - ç”Ÿæˆå™¨å–æ¶ˆ: {is_cancelled}")
            
            monitoring_service.request_end(
                request_id,
                success=False,
                error=str(data),
                response_content="".join(collected_content) if collected_content else None
            )
            await monitoring_service.broadcast_to_monitors({
                "type": "request_end",
                "request_id": request_id,
                "success": False
            })
            yield format_openai_error_chunk(str(data), model, response_id)
            yield format_openai_finish_chunk(model, response_id, reason='stop')
            return # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œå¯ä»¥ç«‹å³ç»ˆæ­¢

    # åªæœ‰åœ¨ _process_lmarena_stream è‡ªç„¶ç»“æŸå (å³æ”¶åˆ° [DONE]) æ‰æ‰§è¡Œ
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨å‘é€ç»“æŸå—å‰ï¼Œæ·»åŠ çŸ­æš‚å»¶è¿Ÿç¡®ä¿æ‰€æœ‰å†…å®¹ç¼“å†²éƒ½å·²åˆ·æ–°
    await asyncio.sleep(0.1)  # 100mså»¶è¿Ÿï¼Œç¡®ä¿æ‰€æœ‰yieldéƒ½å·²å®Œæˆ
    
    # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šåœ¨ä½¿ç”¨input_tokensä¹‹å‰å…ˆè®¡ç®—å®ƒä»¬
    # è®°å½•è¯·æ±‚æˆåŠŸï¼ˆåŒ…å«å“åº”å†…å®¹ï¼‰
    full_response = "".join(collected_content)
    full_reasoning = "".join(reasoning_content) if reasoning_content else None
    
    # ğŸ”§ æ”¹è¿›ï¼šä¼˜å…ˆä½¿ç”¨LMArenaè¿”å›çš„å®é™…tokenæ•°ï¼Œå¦åˆ™ä½¿ç”¨tiktokenç²¾ç¡®è®¡æ•°
    input_tokens = 0
    output_tokens = 0
    
    if lmarena_usage:
        # ä½¿ç”¨LMArenaè¿”å›çš„å®é™…tokenæ•°
        input_tokens = lmarena_usage.get('inputTokens', 0) or lmarena_usage.get('prompt_tokens', 0)
        output_tokens = lmarena_usage.get('outputTokens', 0) or lmarena_usage.get('completion_tokens', 0)
        logger.info(f"[TOKEN_STREAM] ä½¿ç”¨LMArenaå®é™…tokenæ•°: input={input_tokens}, output={output_tokens}")
    else:
        # å›é€€åˆ°æœ¬åœ°tokenizerè®¡æ•°
        # é¦–å…ˆéœ€è¦å¯¼å…¥get_tokenizer_for_modelæ¥ç¡®å®šä½¿ç”¨å“ªç§tokenizer
        from modules.token_counter import get_tokenizer_for_model
        tokenizer_type = get_tokenizer_for_model(model)
        logger.info(f"[TOKEN_STREAM] LMArenaæœªæä¾›tokenä¿¡æ¯ï¼Œä½¿ç”¨{tokenizer_type}è®¡æ•°ï¼ˆæ¨¡å‹: {model}ï¼‰")
        if hasattr(monitoring_service, 'active_requests') and request_id in monitoring_service.active_requests:
            request_info = monitoring_service.active_requests[request_id]
            if request_info.request_messages:
                # ä½¿ç”¨tiktokenè®¡ç®—è¾“å…¥token
                try:
                    input_tokens = estimate_message_tokens(request_info.request_messages, model)
                    logger.info(f"[TOKEN_STREAM] {tokenizer_type}è®¡ç®—è¾“å…¥tokens: {input_tokens}")
                except Exception as e:
                    logger.warning(f"[TOKEN_STREAM] tiktokenè®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—: {e}")
                    # å›é€€åˆ°ç®€å•ä¼°ç®—
                    for msg in request_info.request_messages:
                        if isinstance(msg, dict) and 'content' in msg:
                            content = msg.get('content', '')
                            if isinstance(content, str):
                                input_tokens += len(content) // 4
                            elif isinstance(content, list):
                                for part in content:
                                    if isinstance(part, dict) and part.get('type') == 'text':
                                        input_tokens += len(part.get('text', '')) // 4
        
        # ä½¿ç”¨æœ¬åœ°tokenizerè®¡ç®—è¾“å‡ºtoken
        try:
            output_tokens = estimate_tokens(full_response, model)
            logger.info(f"[TOKEN_STREAM] {tokenizer_type}è®¡ç®—è¾“å‡ºtokens: {output_tokens}")
        except Exception as e:
            logger.warning(f"[TOKEN_STREAM] tiktokenè®¡ç®—è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—: {e}")
            output_tokens = len(full_response) // 4
    
    final_usage = {
        "prompt_tokens": input_tokens,
        "completion_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
    }
    yield format_openai_finish_chunk(model, response_id, reason=finish_reason_to_send, usage=final_usage)
    
    # è¯Šæ–­ï¼šè¾“å‡ºæµå¼è¾“å‡ºç»Ÿè®¡å’Œå†…å®¹æ ¡éªŒ
    if CONFIG.get("debug_stream_timing", False) and chunks_sent > 0:
        total_time = time_module.time() - stream_start_time
        collected_chars = sum(len(c) for c in collected_content)
        logger.info(f"[STREAM_OUTPUT_STATS] è¯·æ±‚ID: {request_id[:8]}")
        logger.info(f"  - å‘é€å—æ•°: {chunks_sent}")
        logger.info(f"  - æ€»å­—ç¬¦æ•°: {collected_chars}")
        logger.info(f"  - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"  - å¹³å‡å‘é€é—´éš”: {total_time/chunks_sent:.3f}ç§’/å—")
    
    # ğŸ”§ å†…å®¹å®Œæ•´æ€§æ ¡éªŒï¼šç¡®ä¿collected_contentä¸ä¸ºç©º
    if not collected_content and not client_disconnected:
        logger.error(f"[STREAM_OUTPUT_STATS] âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•å†…å®¹ï¼å¯èƒ½å­˜åœ¨å†…å®¹ä¸¢å¤±")
    
    # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè®°å½•ç»“æŸçŠ¶æ€
    if client_disconnected:
        logger.warning(f"[DISCONNECT_DETECT] âš ï¸ æµå¼ç”Ÿæˆå™¨å› å®¢æˆ·ç«¯æ–­å¼€è€Œç»“æŸ: {request_id[:8]}")
    elif is_cancelled:
        logger.warning(f"[DISCONNECT_DETECT] âš ï¸ æµå¼ç”Ÿæˆå™¨è¢«å–æ¶ˆ: {request_id[:8]}")
    else:
        logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨æ­£å¸¸ç»“æŸã€‚")

    # è®°å½•è¯·æ±‚æˆåŠŸ
    monitoring_service.request_end(
        request_id,
        success=True,
        response_content=full_response,
        reasoning_content=full_reasoning,
        input_tokens=input_tokens,
        output_tokens=output_tokens
    )
    await monitoring_service.broadcast_to_monitors({
        "type": "request_end",
        "request_id": request_id,
        "success": True
    })


async def non_stream_response(request_id: str, model: str, _process_lmarena_stream_func,
                              format_openai_non_stream_response, CONFIG: dict,
                              response_channels: dict, request_metadata: dict,
                              monitoring_service, estimate_message_tokens, estimate_tokens,
                              release_tab_request, Response):
    """èšåˆå†…éƒ¨äº‹ä»¶æµå¹¶è¿”å›å•ä¸ª OpenAI JSON å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å¼€å§‹å¤„ç†éæµå¼å“åº”ã€‚")
    
    full_content = []
    reasoning_content = []
    finish_reason = "stop"
    lmarena_usage = None  # å­˜å‚¨ä»LMArenaæå–çš„tokenä½¿ç”¨ä¿¡æ¯
    
    # æ€ç»´é“¾é…ç½®
    enable_reasoning_output = CONFIG.get("enable_lmarena_reasoning", False)
    reasoning_mode = CONFIG.get("reasoning_output_mode", "openai")
    
    async for event_type, data in _process_lmarena_stream_func(request_id):
        if event_type == 'retry_info':
            # éæµå¼å“åº”ä¸­è®°å½•é‡è¯•ä¿¡æ¯
            logger.info(f"NON-STREAM [ID: {request_id[:8]}]: é‡è¯•ä¿¡æ¯ - å°è¯• {data.get('attempt')}/{data.get('max_attempts')}")
        elif event_type == 'reasoning' or event_type == 'reasoning_complete':
            # æ”¶é›†æ€ç»´é“¾å†…å®¹
            reasoning_content.append(data)
        elif event_type == 'content':
            full_content.append(data)
        elif event_type == 'finish':
            # dataç°åœ¨æ˜¯ä¸€ä¸ªå­—å…¸: {'reason': ..., 'usage': ...}
            if isinstance(data, dict):
                finish_reason = data.get('reason', 'stop')
                lmarena_usage = data.get('usage')
                if lmarena_usage:
                    logger.info(f"[TOKEN_NON_STREAM] æ•è·åˆ°LMArena tokenä½¿ç”¨ä¿¡æ¯: {lmarena_usage}")
            else:
                # å‘åå…¼å®¹æ—§æ ¼å¼
                finish_reason = data
            
            if finish_reason == 'content-filter':
                full_content.append("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› ")
        elif event_type == 'error':
            logger.error(f"NON-STREAM [ID: {request_id[:8]}]: å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {data}")
            
            monitoring_service.request_end(
                request_id,
                success=False,
                error=str(data),
                response_content="".join(full_content) if full_content else None
            )
            await monitoring_service.broadcast_to_monitors({
                "type": "request_end",
                "request_id": request_id,
                "success": False
            })
            
            # ç»Ÿä¸€æµå¼å’Œéæµå¼å“åº”çš„é”™è¯¯çŠ¶æ€ç 
            status_code = 413 if "é™„ä»¶å¤§å°è¶…è¿‡äº†" in str(data) else 500

            error_response = {
                "error": {
                    "message": f"[LMArena Bridge Error]: {data}",
                    "type": "bridge_error",
                    "code": "attachment_too_large" if status_code == 413 else "processing_error"
                }
            }
            return Response(content=json.dumps(error_response, ensure_ascii=False), status_code=status_code, media_type="application/json")

    # å¤„ç†æ€ç»´é“¾å†…å®¹
    if enable_reasoning_output and reasoning_content:
        full_reasoning = "".join(reasoning_content)
        
        if reasoning_mode == "openai":
            # OpenAIæ¨¡å¼ï¼šæ·»åŠ reasoning_contentå­—æ®µ
            final_content_str = "".join(full_content)
            response_data = {
                "id": response_id,
                "object": "chat.completion",
                "created": int(time.time()),
                "model": model,
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": final_content_str,
                        "reasoning_content": full_reasoning  # æ·»åŠ æ€ç»´é“¾
                    },
                    "finish_reason": finish_reason,
                }],
                "usage": {
                    "prompt_tokens": 0,
                    "completion_tokens": len(final_content_str) // 4,
                    "total_tokens": len(final_content_str) // 4,
                },
            }
        elif reasoning_mode == "think_tag":
            # think_tagæ¨¡å¼ï¼šå°†æ€ç»´é“¾åŒ…è£¹åæ”¾åœ¨contentå‰é¢
            wrapped_reasoning = f"<think>{full_reasoning}</think>\n\n"
            final_content_str = wrapped_reasoning + "".join(full_content)
            response_data = format_openai_non_stream_response(final_content_str, model, response_id, reason=finish_reason)
    else:
        # æ²¡æœ‰å¯ç”¨æ€ç»´é“¾è¾“å‡ºï¼Œæˆ–è€…æ²¡æœ‰æ€ç»´é“¾å†…å®¹ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        final_content_str = "".join(full_content)
        response_data = format_openai_non_stream_response(final_content_str, model, response_id, reason=finish_reason)
    
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å“åº”èšåˆå®Œæˆã€‚")
    
    # ğŸ”§ æ”¹è¿›ï¼šä¼˜å…ˆä½¿ç”¨LMArenaè¿”å›çš„å®é™…tokenæ•°ï¼Œå¦åˆ™ä½¿ç”¨tiktokenç²¾ç¡®è®¡æ•°
    input_tokens = 0
    output_tokens = 0
    
    if lmarena_usage:
        # ä½¿ç”¨LMArenaè¿”å›çš„å®é™…tokenæ•°
        input_tokens = lmarena_usage.get('inputTokens', 0) or lmarena_usage.get('prompt_tokens', 0)
        output_tokens = lmarena_usage.get('outputTokens', 0) or lmarena_usage.get('completion_tokens', 0)
        logger.info(f"[TOKEN_NON_STREAM] ä½¿ç”¨LMArenaå®é™…tokenæ•°: input={input_tokens}, output={output_tokens}")
    else:
        # å›é€€åˆ°æœ¬åœ°tokenizerè®¡æ•°
        from modules.token_counter import get_tokenizer_for_model
        tokenizer_type = get_tokenizer_for_model(model)
        logger.info(f"[TOKEN_NON_STREAM] LMArenaæœªæä¾›tokenä¿¡æ¯ï¼Œä½¿ç”¨{tokenizer_type}è®¡æ•°ï¼ˆæ¨¡å‹: {model}ï¼‰")
        if hasattr(monitoring_service, 'active_requests') and request_id in monitoring_service.active_requests:
            request_info = monitoring_service.active_requests[request_id]
            if request_info.request_messages:
                # ä½¿ç”¨æœ¬åœ°tokenizerè®¡ç®—è¾“å…¥token
                try:
                    input_tokens = estimate_message_tokens(request_info.request_messages, model)
                    logger.info(f"[TOKEN_NON_STREAM] {tokenizer_type}è®¡ç®—è¾“å…¥tokens: {input_tokens}")
                except Exception as e:
                    logger.warning(f"[TOKEN_NON_STREAM] tiktokenè®¡ç®—å¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—: {e}")
                    # å›é€€åˆ°ç®€å•ä¼°ç®—
                    for msg in request_info.request_messages:
                        if isinstance(msg, dict) and 'content' in msg:
                            content = msg.get('content', '')
                            if isinstance(content, str):
                                input_tokens += len(content) // 4
                            elif isinstance(content, list):
                                for part in content:
                                    if isinstance(part, dict) and part.get('type') == 'text':
                                        input_tokens += len(part.get('text', '')) // 4
        
        # è®¡ç®—å®Œæ•´å“åº”å†…å®¹ï¼ˆåŒ…æ‹¬æ€ç»´é“¾ï¼‰
        full_response_for_monitoring = final_content_str if 'final_content_str' in locals() else "".join(full_content)
        
        # ä½¿ç”¨æœ¬åœ°tokenizerè®¡ç®—è¾“å‡ºtoken
        try:
            output_tokens = estimate_tokens(full_response_for_monitoring, model)
            logger.info(f"[TOKEN_NON_STREAM] {tokenizer_type}è®¡ç®—è¾“å‡ºtokens: {output_tokens}")
        except Exception as e:
            logger.warning(f"[TOKEN_NON_STREAM] tiktokenè®¡ç®—è¾“å‡ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼°ç®—: {e}")
            output_tokens = len(full_response_for_monitoring) // 4
    
    # è®¡ç®—å®Œæ•´å“åº”å†…å®¹ï¼ˆåŒ…æ‹¬æ€ç»´é“¾ï¼‰
    full_response_for_monitoring = final_content_str if 'final_content_str' in locals() else "".join(full_content)
    full_reasoning_for_monitoring = "".join(reasoning_content) if reasoning_content else None
    
    # æ›´æ–°å“åº”ä¸­çš„usageå­—æ®µ
    response_data['usage'] = {
        "prompt_tokens": input_tokens,
        "completion_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
    }
    
    monitoring_service.request_end(
        request_id,
        success=True,
        response_content=full_response_for_monitoring,
        reasoning_content=full_reasoning_for_monitoring,
        input_tokens=input_tokens,
        output_tokens=output_tokens
    )
    
    # ğŸ”§ å…³é”®ä¿®å¤ï¼šé‡Šæ”¾æ ‡ç­¾é¡µè¯·æ±‚è®¡æ•°
    if request_id in request_metadata:
        tab_id = request_metadata[request_id].get("tab_id")
        if tab_id:
            await release_tab_request(tab_id)
            logger.debug(f"NON-STREAM [ID: {request_id[:8]}]: å·²é‡Šæ”¾æ ‡ç­¾é¡µ '{tab_id}' çš„è¯·æ±‚è®¡æ•°")
    
    return Response(content=json.dumps(response_data, ensure_ascii=False), media_type="application/json")