"""
åŽå°ç›‘æŽ§ä»»åŠ¡
åŒ…å«å†…å­˜ç›‘æŽ§ã€é…ç½®æ–‡ä»¶ç›‘æŽ§ã€æ´»è·ƒè¯·æ±‚æ¸…ç†ç­‰
"""
import asyncio
import gc
import logging
import os
import psutil
import time
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)


async def stale_request_cleaner(monitoring_service):
    """
    æ ¸å¿ƒä¿®å¤ï¼šå®šæœŸæ¸…ç†è¶…æ—¶çš„æ´»è·ƒè¯·æ±‚
    é˜²æ­¢è¯·æ±‚å› å¼‚å¸¸è€Œæ°¸ä¹…å¡åœ¨"å¤„ç†ä¸­"çŠ¶æ€
    """
    logger.info("[STALE_CLEANER] æ´»è·ƒè¯·æ±‚æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨")
    
    while True:
        try:
            # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            await asyncio.sleep(60)
            
            # è°ƒç”¨ç›‘æŽ§æœåŠ¡çš„æ¸…ç†å‡½æ•°
            cleaned_count = monitoring_service.cleanup_stale_requests()
            
            if cleaned_count > 0:
                logger.warning(f"[STALE_CLEANER] âš ï¸ æ¸…ç†äº† {cleaned_count} ä¸ªè¶…æ—¶çš„æ´»è·ƒè¯·æ±‚")
                
                # å¹¿æ’­æ¸…ç†äº‹ä»¶åˆ°ç›‘æŽ§é¢æ¿
                await monitoring_service.broadcast_to_monitors({
                    "type": "stale_requests_cleaned",
                    "count": cleaned_count,
                    "timestamp": time.time()
                })
            else:
                # æ­£å¸¸æƒ…å†µï¼Œè®°å½•DEBUGæ—¥å¿—
                logger.debug(f"[STALE_CLEANER] æ£€æŸ¥å®Œæˆï¼Œå½“å‰æ´»è·ƒè¯·æ±‚: {len(monitoring_service.active_requests)}")
                
        except Exception as e:
            logger.error(f"[STALE_CLEANER] é”™è¯¯: {e}", exc_info=True)


async def config_monitor(CONFIG, CONFIG_FILE_MTIMES, load_config_func, load_model_endpoint_map_func, load_model_map_func, browser_connections, response_channels, MODEL_ENDPOINT_MAP):
    """å®šæœŸç›‘æŽ§é…ç½®æ–‡ä»¶çš„å˜åŒ–å¹¶æŠ¥å‘Š"""
    logger.info("[CONFIG_MONITOR] é…ç½®æ–‡ä»¶ç›‘æŽ§ä»»åŠ¡å·²å¯åŠ¨")
    
    while True:
        try:
            # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡é…ç½®æ–‡ä»¶
            await asyncio.sleep(30)
            
            current_time = time.time()
            config_changes = []
            
            # æ£€æŸ¥ config.jsonc
            try:
                config_mtime = os.path.getmtime('config.jsonc')
                if config_mtime != CONFIG_FILE_MTIMES.get('config.jsonc', 0):
                    old_mtime = CONFIG_FILE_MTIMES.get('config.jsonc', 0)
                    # è°ƒç”¨ load_config() ä¼šæ›´æ–° CONFIG_FILE_MTIMES
                    load_config_func()
                    config_changes.append(f"config.jsonc (ä¿®æ”¹äºŽ {datetime.fromtimestamp(config_mtime).strftime('%H:%M:%S')})")
            except FileNotFoundError:
                pass
            
            # æ£€æŸ¥ model_endpoint_map.json
            try:
                map_mtime = os.path.getmtime('model_endpoint_map.json')
                if map_mtime != CONFIG_FILE_MTIMES.get('model_endpoint_map.json', 0):
                    old_mtime = CONFIG_FILE_MTIMES.get('model_endpoint_map.json', 0)
                    # è°ƒç”¨ load_model_endpoint_map() ä¼šæ›´æ–° CONFIG_FILE_MTIMES
                    load_model_endpoint_map_func()
                    config_changes.append(f"model_endpoint_map.json (ä¿®æ”¹äºŽ {datetime.fromtimestamp(map_mtime).strftime('%H:%M:%S')})")
            except FileNotFoundError:
                pass
            
            # æ£€æŸ¥ models.json
            try:
                models_mtime = os.path.getmtime('models.json')
                if models_mtime != CONFIG_FILE_MTIMES.get('models.json', 0):
                    old_mtime = CONFIG_FILE_MTIMES.get('models.json', 0)
                    # é‡æ–°åŠ è½½ models.json
                    load_model_map_func()
                    CONFIG_FILE_MTIMES['models.json'] = models_mtime
                    config_changes.append(f"models.json (ä¿®æ”¹äºŽ {datetime.fromtimestamp(models_mtime).strftime('%H:%M:%S')})")
            except FileNotFoundError:
                pass
            
            # å¦‚æžœæœ‰é…ç½®å˜åŒ–ï¼ŒæŠ¥å‘Šæ—¥å¿—
            if config_changes:
                logger.info(f"[CONFIG_MONITOR] ðŸ”„ æ£€æµ‹åˆ°é…ç½®æ–‡ä»¶æ›´æ–°: {', '.join(config_changes)}")
                logger.info(f"[CONFIG_MONITOR] âœ… é…ç½®å·²è‡ªåŠ¨é‡æ–°åŠ è½½")
            else:
                # å®šæœŸæŠ¥å‘ŠçŠ¶æ€ï¼ˆç±»ä¼¼å†…å­˜ç›‘æŽ§ï¼‰
                logger.debug(f"[CONFIG_MONITOR] é…ç½®æ–‡ä»¶æ— å˜åŒ– | "
                           f"browser_connections: {len(browser_connections)} | "
                           f"response_channels: {len(response_channels)} | "
                           f"model_endpoints: {len(MODEL_ENDPOINT_MAP)}")
            
        except Exception as e:
            logger.error(f"[CONFIG_MONITOR] é”™è¯¯: {e}", exc_info=True)


async def memory_monitor(
    CONFIG,
    DOWNLOAD_SEMAPHORE,
    MAX_CONCURRENT_DOWNLOADS,
    response_channels,
    request_metadata,
    IMAGE_BASE64_CACHE,
    FILEBED_URL_CACHE,
    FILEBED_URL_CACHE_TTL,
    downloaded_urls_set,
    downloaded_image_urls
):
    """ä¼˜åŒ–çš„å†…å­˜ç›‘æŽ§ä»»åŠ¡"""
    process = psutil.Process(os.getpid())
    last_gc_time = time.time()
    
    while True:
        try:
            # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹çš„ç›‘æŽ§ï¼‰
            await asyncio.sleep(60)
            
            # èŽ·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / (1024 * 1024)
            
            # èŽ·å–ä¸‹è½½å¹¶å‘çŠ¶æ€
            active_downloads = MAX_CONCURRENT_DOWNLOADS - DOWNLOAD_SEMAPHORE._value if DOWNLOAD_SEMAPHORE else 0
            
            # è®°å½•å†…å­˜çŠ¶æ€ï¼ˆæ›´è¯¦ç»†çš„ä¿¡æ¯ï¼‰
            logger.info(f"[MEM_MONITOR] å†…å­˜: {memory_mb:.2f}MB | "
                       f"æ´»è·ƒä¸‹è½½: {active_downloads}/{MAX_CONCURRENT_DOWNLOADS} | "
                       f"å“åº”é€šé“: {len(response_channels)} | "
                       f"è¯·æ±‚å…ƒæ•°æ®: {len(request_metadata)} | "
                       f"ç¼“å­˜å›¾ç‰‡: {len(IMAGE_BASE64_CACHE)} | "
                       f"å›¾åºŠURLç¼“å­˜: {len(FILEBED_URL_CACHE)} | "
                       f"ä¸‹è½½åŽ†å²: {len(downloaded_urls_set)}")
            
            # æ–°å¢žï¼šæ¸…ç†è¿‡æœŸçš„å›¾åºŠURLç¼“å­˜
            if len(FILEBED_URL_CACHE) > 0:
                current_time = time.time()
                expired_hashes = []
                for img_hash, (url, cache_time) in FILEBED_URL_CACHE.items():
                    if current_time - cache_time > FILEBED_URL_CACHE_TTL:
                        expired_hashes.append(img_hash)
                
                if expired_hashes:
                    for img_hash in expired_hashes:
                        del FILEBED_URL_CACHE[img_hash]
                    logger.info(f"[MEM_MONITOR] æ¸…ç†äº† {len(expired_hashes)} ä¸ªè¿‡æœŸçš„å›¾åºŠURLç¼“å­˜")
            
            # æ–°å¢žï¼šç›‘æŽ§å’Œæ¸…ç†è¶…æ—¶çš„è¯·æ±‚å…ƒæ•°æ®
            if len(request_metadata) > 10:  # å¦‚æžœå…ƒæ•°æ®è¿‡å¤šï¼Œå¯èƒ½æœ‰å†…å­˜æ³„æ¼
                logger.warning(f"[MEM_MONITOR] request_metadataæ•°é‡è¾ƒå¤š: {len(request_metadata)}")
                logger.warning(f"[MEM_MONITOR] å¼€å§‹æ¸…ç†è¶…æ—¶çš„è¯·æ±‚å…ƒæ•°æ®...")
                
                # å®žçŽ°è¶…æ—¶æ¸…ç†é€»è¾‘
                current_time = datetime.now()
                timeout_threshold = CONFIG.get("metadata_timeout_minutes", 30)  # é»˜è®¤30åˆ†é’Ÿè¶…æ—¶
                stale_request_ids = []
                
                for req_id, metadata in request_metadata.items():
                    created_at_str = metadata.get("created_at")
                    if created_at_str:
                        try:
                            created_at = datetime.fromisoformat(created_at_str)
                            age_minutes = (current_time - created_at).total_seconds() / 60
                            
                            if age_minutes > timeout_threshold:
                                stale_request_ids.append(req_id)
                                logger.info(f"[MEM_MONITOR] å‘çŽ°è¶…æ—¶å…ƒæ•°æ®: {req_id[:8]} (å­˜æ´»: {age_minutes:.1f}åˆ†é’Ÿ)")
                        except (ValueError, TypeError) as e:
                            logger.warning(f"[MEM_MONITOR] æ— æ³•è§£æžå…ƒæ•°æ®æ—¶é—´: {req_id[:8]}, é”™è¯¯: {e}")
                            stale_request_ids.append(req_id)  # æ— æ•ˆæ—¶é—´æˆ³ä¹Ÿæ¸…ç†
                
                # æ¸…ç†è¶…æ—¶çš„å…ƒæ•°æ®
                for req_id in stale_request_ids:
                    del request_metadata[req_id]
                    # åŒæ—¶æ¸…ç†å¯¹åº”çš„å“åº”é€šé“ï¼ˆå¦‚æžœè¿˜å­˜åœ¨ï¼‰
                    if req_id in response_channels:
                        del response_channels[req_id]
                        logger.debug(f"[MEM_MONITOR] ä¸€å¹¶æ¸…ç†å“åº”é€šé“: {req_id[:8]}")
                
                if stale_request_ids:
                    logger.info(f"[MEM_MONITOR] å·²æ¸…ç† {len(stale_request_ids)} ä¸ªè¶…æ—¶çš„è¯·æ±‚å…ƒæ•°æ®")
                else:
                    logger.info(f"[MEM_MONITOR] æœªå‘çŽ°è¶…æ—¶å…ƒæ•°æ®ï¼Œä½†æ•°é‡ä»ç„¶è¾ƒå¤šï¼Œå¯èƒ½æ˜¯æ­£å¸¸æƒ…å†µ")
            else:
                logger.debug(f"[MEM_MONITOR] request_metadata: {len(request_metadata)}")
            
            # ä»Žé…ç½®è¯»å–å†…å­˜ç®¡ç†é˜ˆå€¼
            mem_config = CONFIG.get("memory_management", {})
            gc_threshold = mem_config.get("gc_threshold_mb", 500)
            cache_config = mem_config.get("cache_config", {})
            
            # æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´
            if memory_mb > gc_threshold:
                current_time = time.time()
                # é˜²æ­¢è¿‡äºŽé¢‘ç¹çš„GC
                if current_time - last_gc_time > 300:  # 5åˆ†é’Ÿæœ€å¤šGCä¸€æ¬¡
                    logger.warning(f"[MEM_MONITOR] è§¦å‘åžƒåœ¾å›žæ”¶ (å†…å­˜: {memory_mb:.2f}MB > {gc_threshold}MB)")
                    
                    # æ¸…ç†å›¾ç‰‡ç¼“å­˜
                    cache_max = cache_config.get("image_cache_max_size", 500)
                    cache_keep = cache_config.get("image_cache_keep_size", 200)
                    if len(IMAGE_BASE64_CACHE) > cache_max:
                        # ä¿ç•™æœ€æ–°çš„æŒ‡å®šæ•°é‡
                        sorted_items = sorted(IMAGE_BASE64_CACHE.items(),
                                            key=lambda x: x[1][1], reverse=True)
                        IMAGE_BASE64_CACHE.clear()
                        for url, data in sorted_items[:cache_keep]:
                            IMAGE_BASE64_CACHE[url] = data
                        logger.info(f"[MEM_MONITOR] æ¸…ç†å›¾ç‰‡ç¼“å­˜: {len(sorted_items)} -> {cache_keep}")
                    
                    # æ¸…ç†ä¸‹è½½è®°å½•
                    url_history_max = cache_config.get("url_history_max", 2000)
                    url_history_keep = cache_config.get("url_history_keep", 1000)
                    if len(downloaded_urls_set) > url_history_max:
                        downloaded_urls_set.clear()
                        # ä¿ç•™æœ€è¿‘çš„è®°å½•
                        downloaded_urls_set.update(list(downloaded_image_urls)[-url_history_keep:])
                        logger.info(f"[MEM_MONITOR] æ¸…ç†ä¸‹è½½è®°å½•: {url_history_max} -> {url_history_keep}")
                    
                    # æ‰§è¡Œåžƒåœ¾å›žæ”¶
                    gc.collect()
                    last_gc_time = current_time
                    
                    # å¼ºåˆ¶åˆ·æ–°è¿›ç¨‹å¯¹è±¡å¹¶å†æ¬¡æ£€æŸ¥å†…å­˜
                    # æ ¸å¿ƒä¿®å¤ï¼šé‡æ–°åˆ›å»ºProcesså¯¹è±¡ä»¥èŽ·å–æœ€æ–°å†…å­˜ä¿¡æ¯
                    fresh_process = psutil.Process(os.getpid())
                    new_memory_mb = fresh_process.memory_info().rss / (1024 * 1024)
                    logger.info(f"[MEM_MONITOR] GCåŽå†…å­˜: {memory_mb:.2f}MB -> {new_memory_mb:.2f}MB "
                               f"(é‡Šæ”¾: {memory_mb - new_memory_mb:.2f}MB)")
                    
        except Exception as e:
            logger.error(f"[MEM_MONITOR] é”™è¯¯: {e}")